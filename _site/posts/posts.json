[
  {
    "path": "posts/2023-05-09-making-post-requests-with-r/",
    "title": "Making POST requests with R",
    "description": "An alternative to RSelenium to get data from dynamic webpages.",
    "author": [
      {
        "name": "Etienne Bacher",
        "url": {}
      }
    ],
    "date": "2023-05-09",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nPOST and GET requests: what?\r\nRequests and responses\r\nGET requests\r\nPOST requests\r\n\r\nMaking POST requests from R\r\nExample\r\nStep 1: find the POST request parameters\r\nStep 2: build the query\r\nStep 3: perform the query\r\nStep 4: extract the information\r\n\r\nBonus\r\n\r\nPreview picture from Kristina Tripkovic on Unsplash\r\n\r\nA few months ago, I gave a training on RSelenium, a package that allows us to\r\nreproduce all the actions we do in a browser (open a website, scroll, click on a\r\nbutton, etc.) with some R code. This is very useful to perform some webscraping on pages that are dynamically updated, meaning that we can’t simply use the URL to get the HTML we want. I won’t explain the basics of webscraping here, there are plenty of articles on this already and the vignettes on the rvest website are very good.\r\nSomething I mentioned very briefly in my RSelenium slides is that, in some cases, it is possible to avoid using RSelenium at all by performing the POST requests ourselves1. However, a few days ago, another case of dynamic webscraping came to me and I thought that maybe that was a good opportunity to try doing this first. But before diving into this example, let’s explain a bit more POST (and GET) requests and why using them directly could save a lot of time.\r\nPOST and GET requests: what?\r\nThere are already a lot of resources on POST and GET requests, like this W3schools page, so here I will only do a brief summary.\r\nRequests and responses\r\nWhat happens when we click on a button, open a new link, or other actions? The website sends a request to the server, the server sends a response back to us, and the website is updated using this new data that the server provided. There are several types of requests, the two most common being GET and POST requests.\r\nBoth GET and POST are used to request data but they have key differences.\r\nGET requests\r\nOne important thing to note in GET requests is that the query parameters (which describe what data we want to obtain) are displayed in the URL after a question mark ?. An example of GET requests is the World Bank API. Using an example on their website, we can start from this query to get the data for a specific indicator and all countries:\r\nhttp://api.worldbank.org/v2/country/all/indicator/SP.POP.TOTL\r\nBy default, this will show the data in the XML format. If we want to use the JSON format instead, we can add a parameter format in the URL:\r\nhttp://api.worldbank.org/v2/country/all/indicator/SP.POP.TOTL?format=json\r\nThis type of requests changes the URL, so this is not very useful in our case where the URL doesn’t change at all.\r\nPOST requests\r\nIn a POST request, the website also sends information to the server and the server also sends data back to us but this time the URL doesn’t change, the website is dynamically updated. This is often used when we fill a form online: depending on what the user provided, the data sent back by the server is not the same.\r\nBasically, using RSelenium for dynamic webscraping boils down to go to the right webpage, fill the inputs so that a POST request is performed behind the scenes, and then scrape the data that the server provided.\r\nBut what if we could avoid doing all these steps by performing the POST requests directly from R? That would be quicker and we would also avoid all issues related to RSelenium (which can be a pain to properly set up). For this to work, we need to know the detailed request to send to the server.\r\nMaking POST requests from R\r\nThere are two main packages to do this: httr and httr2. The latter is just a couple of years old and has being developed with a lot of convenient built-in features (retries if a query failed, secure secrets, etc.2).\r\nWe don’t need very advanced features here so let’s use httr. This package provides a function POST() that (surprise!) allows us to make POST requests. Let’s explore this with an example.\r\nExample\r\nThe Spanish Office of Patents and Brands3 has a nice website where you can get historical data about patents and brands from about mid-19th century to mid-20th century. You can type anything you want in the search bar (e.g a city name), specify the years range, and it provides a list of patents, their title, the exact date at which they were done, and some information on the person or company that made the patent.\r\nNote: this website is in HTTP only, not HTTPS. Therefore, if you use Firefox with the mode HTTPS Everywhere enabled, you need to put http://historico.oepm.es in the list of exceptions (see here).\r\nIn the output table, we can see a “+” button for each row. Clicking on this button displays more detailed information for the patent (e.g the category it belongs to). Ideally, what we would like is to obtain this detailed information for all records. This had less to do with POST requests so I show how to do this in the “Bonus” section at the end.\r\nThis website is dynamically updated, meaning that we have to use RSelenium or reproduce POST requests. These are the packages we will use:\r\n\r\n\r\nlibrary(httr)       # make POST requests\r\nlibrary(polite)     # be polite when we scrape\r\nlibrary(rvest)      # extract HTML tables\r\nlibrary(data.table) # for some data cleaning\r\n\r\n\r\nStep 1: find the POST request parameters\r\nTo see the requests and responses, we can use the tab “Network” in the developer console. Fill the search input (for example type “Madrid”), open the network tab and then click on the magnifying glass icon to run the query. You can see what we send to the server and what the server sends back:\r\n\r\nWe see four lines:\r\nsearch_lib.php\r\nSpanish.json\r\ntwo PNG files\r\nWe already know that the information we want is not the images so we can discard the last two lines. The Spanish.json is intriguing, maybe the data we want is in this JSON? When we click on this request, we see the full URL, but if we open it in another tab, it turns out that this JSON only contains useless information like the loading message.\r\nSo there’s only search_lib.php left, what we’re looking for must be in there. We see several things when we click on it. First, we see that it is a POST request but if we open the URL, it says “Invalid arguments”. This makes sense, there’s nothing unique in this URL, nothing that would allow the server to know the records it should send back.\r\nWe can see a “Payload” tab when we click on the request and it seems that this is where our parameters are stored:\r\n\r\nStep 2: build the query\r\nGreat, but how do we know the exact query? Well, the parameters are displayed in a cleaned way, but the button “View source” shows the raw query:\r\ncadena=madrid&tb=SPH_MATCH_ALL&rangoa=1826%2C1966&indexes%5B%5D=privilegios&\r\nindexes%5B%5D=patentes&indexes%5B%5D=patentes_upm&indexes%5B%5D=marcas&\r\ntimestamp=Mon May 08 2023 15:38:09 GMT+0200 (Central European Summer Time)\r\nWe can now replace the city, years or patent types in the query (the timestamp is useless so I remove it):\r\n\r\n\r\ncity <- \"madrid\"\r\nyear1 <- 1850\r\nyear2 <- 1870\r\n\r\nquery <- paste0(\r\n  \"cadena=\", city, \"&tb=SPH_MATCH_ALL&rangoa=\", year1, \"%2C\", year2,\r\n  \"&indexes%5B%5D=privilegios&indexes%5B%5D=patentes&indexes%5B%5D\",\r\n  \"=patentes_upm&indexes%5B%5D=marcas\"\r\n)\r\n\r\n\r\nStep 3: perform the query\r\nWe now have:\r\nthe base URL that the website contacts to get the data\r\nthe parameters in raw format\r\nThe next step is to actually perform the query:\r\n\r\n\r\n# use politely() to tell the website who is performing the requests and to add\r\n# a delay between requests (here we only do one)\r\npolite_POST <- politely(POST, verbose=TRUE) \r\n\r\nPOST_response <- polite_POST(\r\n  \"http://historico.oepm.es/logica/search_lib.php\",\r\n  add_headers(\r\n    \"accept\" = \"*/*\",\r\n    \"accept-language\" = \"en-GB,en-US;q=0.9,en;q=0.8\",\r\n    \"content-type\" = \"application/x-www-form-urlencoded; charset=UTF-8\",\r\n    \"x-requested-with\" = \"XMLHttpRequest\"\r\n  ),\r\n  body = query\r\n)\r\n\r\n\r\nYou can see additional information in add_headers(). This information is available in the “Headers” tab when the click on the request. However, I don’t really know why some of these headers are necessary and others are not. Note that you can easily get these headers by doing a right-click on the POST request (on the left of the “Network” tab) > “Copy value” > “Copy as fetch” and then pasting it anywhere.\r\nStep 4: extract the information\r\nWe can extract the HTML from the response with content():\r\n\r\n\r\ncontent(POST_response, \"parsed\")\r\n\r\n{html_document}\r\n<html>\r\n[1] <body><div class=\"rs1 row space-bottom\">\\n<div class=\"col-md-10 ...\r\n\r\nThis is the HTML code from the “Mostrando xxx de xxx resultados encontrados” to the bottom of the page. We only want the table, so we extract it with rvest::html_table():\r\n\r\n\r\ncontent(POST_response, \"parsed\") |> \r\n  html_table()\r\n\r\n[[1]]\r\n# A tibble: 12 × 7\r\n   ``    TIPO       SUBTIPO       EXPEDIENTE FECHA DENOMINACION_TITULO\r\n   <lgl> <chr>      <chr>              <int> <chr> <chr>              \r\n 1 NA    Marca      \"\"                   103 1870… La Deliciosa       \r\n 2 NA    Marca      \"Marca de Fá…         54 1867… Fuente de los Cana…\r\n 3 NA    Marca      \"Marca de Fá…         50 1868… Campanadas para in…\r\n 4 NA    Marca      \"Marca de Fá…         66 1868… Compañía Española  \r\n 5 NA    Marca      \"Marca de Fá…         76 1869… Tinta Universal    \r\n 6 NA    Marca      \"Marca de Fá…         80 1869… La Cruz de Puerta …\r\n 7 NA    Marca      \"Marca de Fá…         82 1869… Chocolate de la Co…\r\n 8 NA    Marca      \"Marca de Fá…          6 1866… La Corza           \r\n 9 NA    Privilegio \"Privilegio …       1261 1855… MODO DE FABRICAR U…\r\n10 NA    Privilegio \"Privilegio …       2015 1860… PROCEDIMIENTO PARA…\r\n11 NA    Privilegio \"Privilegio …       2368 1861… PROCEDIMIENTO PARA…\r\n12 NA    Privilegio \"Privilegio …       2631 1863… PROCEDIMIENTO QUIM…\r\n# ℹ 1 more variable: SOLICITANTE <chr>\r\n\r\nAnd voilà! You can now customize the query to loop through cities, years, etc. Note that by default, the number of records returned is limited to 250, and we can’t bypass this.\r\nRemember: use the package polite to avoid flooding the server with requests. As they say on polite’s website, the four pillars of polite session are\r\n\r\nIntroduce Yourself, Seek Permission, Take Slowly and Never Ask Twice.\r\n\r\nBonus\r\nYou know how to make POST requests, but at the beginning of the “Example” section, I said that I wanted the detailed information that is only displayed when we click on a “+” button in the table.\r\nUsing the same method as before, we see that clicking on one of these buttons triggers a GET request of the form ficha.php?id=<ID>&db=<DB>. Problem: these <ID> and <DB> change for each record and their value cannot be easily guessed. However, if we look at the HTML of these buttons, we can see that <ID> and <DB> are stored as attributes of the <a> tag:\r\n\r\nSince the have the HTML of the full table (thanks to the POST request), we can extract these attributes with html_nodes():\r\n\r\n\r\n# get all the attributes for all \"+\" buttons\r\nlist_attrs <- content(POST_response, \"parsed\") |> \r\n  html_nodes(\"td > a\") |> \r\n  html_attrs()\r\n\r\n# for each \"+\" button, extract only the id and db attributes\r\ninfo <- lapply(list_attrs, function(x) {\r\n  out <- x[names(x) %in% c(\"data-id\", \"data-db\")]\r\n  if (length(out) == 0) return(NULL)\r\n  data.frame(id = out[1], db = out[2])\r\n})\r\n\r\n# remove cases where there are no attributes\r\ninfo <- Filter(Negate(is.null), info)\r\n\r\n# transform the list into a clean dataframe\r\nout <- rbindlist(info)\r\nhead(out)\r\n\r\n    id     db\r\n1:   6 maruam\r\n2: 130 maruam\r\n3: 461 maruam\r\n4: 523 maruam\r\n5: 560 maruam\r\n6: 581 maruam\r\n\r\nWe now have a very clean table with the id and db values for each row of the HTML table. We can now make a loop through all of these to create the URL and read it with rvest (I only do it once here to avoid making useless requests):\r\n\r\n\r\nread_html(\r\n  paste0(\"http://historico.oepm.es/logica/ficha.php?id=\", \r\n         6, \"&db=\", \"maruam\")\r\n) |> \r\n  html_table()\r\n\r\n[[1]]\r\n# A tibble: 14 × 2\r\n   X1                             X2                                  \r\n   <chr>                          <chr>                               \r\n 1 Número de Marca                \"103\"                               \r\n 2 Denominación Breve             \"La Deliciosa\"                      \r\n 3 Fecha Solicitud                \"27-10-1870\"                        \r\n 4 Fecha Concesión                \"24-03-1871\"                        \r\n 5 Fecha de publicación Concesión \"\"                                  \r\n 6 Clasificación NIZA             \"CLASE31\"                           \r\n 7 Caducidad                      \"Caducada\"                          \r\n 8 Fecha Caducidad                \"24-03-1891\"                        \r\n 9 Cesiones                       \"Sí\"                                \r\n10 Propietario                    \"Castellá, Joaquín\"                 \r\n11 Lugar de residencia            \"Madrid\"                            \r\n12 Provincia de residencia        \"Madrid\"                            \r\n13 País de residencia             \"ESPAÑA\"                            \r\n14 Profesión                      \"Fabricante, industriales y empresa…\r\n\r\nThere’s still some cleaning to do but this post is quite dense so I’ll stop here. The full code (POST + GET + cleaning) can be found on this repo, in the file “demo.R”: webscraping-patents-spain. Thanks for having read so far!\r\n\r\nIn fact, I completely buried this in the Appendix because RSelenium was already plenty to teach and I didn’t want to add another layer of complexity with POST requests.↩︎\r\nI’m just listing things that are written on the website of httr2, more details there.↩︎\r\nOficina Española de Patentes y Marcas.↩︎\r\n",
    "preview": "posts/2023-05-09-making-post-requests-with-r/post-photo.jpg",
    "last_modified": "2023-05-09T14:52:51+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-03-01-do-bonus-points-lead-to-more-tries-in-6-nations-matches/",
    "title": "Do bonus points lead to more tries in 6 Nations matches?",
    "description": "A small post where rugby is an excuse to do some webscraping and some\n`ggplot`-ing.",
    "author": [
      {
        "name": "Etienne Bacher",
        "url": {}
      }
    ],
    "date": "2023-03-01",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nGetting the data\r\nPlotting the data\r\n\r\nThe 6 Nations is a rugby tournament that takes place every year in February-March\r\nbetween the six strongest national teams in Europe: England, France, Ireland,\r\nItaly, Scotland, and Wales. Each team plays against the other five. A victory gives 4 points, a draw gives 2 points, and a loss gives 0 point.\r\nIn 2017, bonus points were introduced:\r\na try bonus point: you get one extra point if you score 4 tries or more during\r\nthe match, whatever the final result;\r\na losing bonus point: you get one extra point if you lose the match by 7\r\npoints or less.\r\nTherefore, a victory can now give you 5 points maximum and a loss can give you\r\n1 point. Additionally, a team that makes the Grand Slam (wins all 5 matches)\r\ngets a bonus of 3 points1.\r\nThe idea behind these new rules were to improve the drama by pushing teams to\r\nscore more tries. In this post, I’d like to check if we saw an increase in the\r\nnumber of tries since 2017.\r\nI’m not going to make a deep exploration or to find whether or not there is a\r\ntrue causal effect between this new rule and the number of tries. It’s just a\r\ngood pretext to do some scraping and some graphs.\r\nGetting the data\r\n\r\n\r\nlibrary(rvest)\r\nlibrary(tidyverse)\r\nlibrary(patchwork)\r\n\r\n\r\nI didn’t find a clean dataset with the results for the latest editions, so we\r\ncan scrape the ESPN website instead. It only goes back to 2008 but it will do the\r\njob.\r\nThe URLs are identical except for the tournament year. Once we get the HTML for\r\none year, we can extract the table with its CSS classes, and then format it\r\nnicely with html_table().\r\n\r\n\r\nresults <- list()\r\nfor (i in 2008:2022) {\r\n  print(paste0(\"Scraping year \", i))\r\n  results[[i]] <- read_html(\r\n    paste0(\"https://www.espn.co.uk/rugby/table/_/league/180659/season/\", i)\r\n  ) |> \r\n    html_element(css = \".standings .has-team-logos\") |> \r\n    html_table() |> \r\n    mutate(year = i)\r\n}\r\n\r\n[1] \"Scraping year 2008\"\r\n[1] \"Scraping year 2009\"\r\n[1] \"Scraping year 2010\"\r\n[1] \"Scraping year 2011\"\r\n[1] \"Scraping year 2012\"\r\n[1] \"Scraping year 2013\"\r\n[1] \"Scraping year 2014\"\r\n[1] \"Scraping year 2015\"\r\n[1] \"Scraping year 2016\"\r\n[1] \"Scraping year 2017\"\r\n[1] \"Scraping year 2018\"\r\n[1] \"Scraping year 2019\"\r\n[1] \"Scraping year 2020\"\r\n[1] \"Scraping year 2021\"\r\n[1] \"Scraping year 2022\"\r\n\r\nWe can then aggregate this list into a single dataframe.\r\n\r\n\r\nall_results <- data.table::rbindlist(results) |>\r\n  rename(country = \"Six Nations\") |> \r\n  mutate(\r\n    country = case_when(\r\n      grepl(\"Wales\", country) ~ \"Wales\",\r\n      grepl(\"England\", country) ~ \"England\",\r\n      grepl(\"Italy\", country) ~ \"Italy\",\r\n      grepl(\"France\", country) ~ \"France\",\r\n      grepl(\"Scotland\", country) ~ \"Scotland\",\r\n      grepl(\"Ireland\", country) ~ \"Ireland\"\r\n    )\r\n  )\r\n\r\n\r\nPlotting the data\r\nFirst, let’s see the total number of tries for each year.\r\n\r\n\r\nFONT <- \"Cinzel\"\r\n\r\nshowtext::showtext_auto()\r\nsysfonts::font_add_google(FONT)\r\n\r\ntheme_custom <- function(...) {\r\n  theme_light() +\r\n  theme(\r\n    panel.grid.minor = element_blank(),\r\n    text = element_text(family = FONT)\r\n  )\r\n}\r\n\r\nlabs <- list(\r\n  x = \"Year\",\r\n  y = \"Tries per year\"\r\n)\r\n\r\nall_results |> \r\n  summarise(tries_per_year = sum(TF), .by = year) |> \r\n  ggplot(aes(year, tries_per_year)) +\r\n  geom_point(color = \"black\", fill = \"#99b3e6\", shape = 21, size = 2.5) +\r\n  geom_vline(xintercept = 2016.5, linetype = \"dashed\") +\r\n  ylim(c(0, 100)) +\r\n  labs(\r\n    title = \"Number of tries per tournament\",\r\n    x = labs$x,\r\n    y = labs$y\r\n  ) +\r\n  theme_custom() +\r\n  theme(\r\n    axis.title = element_text(size = 28),\r\n    axis.text = element_text(size = 25),\r\n    plot.title = element_text(size = 35)\r\n  )\r\n\r\n\r\n\r\nWe see an increase in the number of tries, but this upward trend started before\r\n2017, putting into question the real causal effect of this new rule. Was this\r\nincrease similar for all countries?\r\n\r\n\r\nplots <- list()\r\nfor (i in unique(all_results$country)) {\r\n  \r\n  main_color <- switch(i,\r\n    \"Ireland\" = \"#339966\",\r\n    \"France\" = \"#0044cc\",\r\n    \"England\" = \"white\",\r\n    \"Wales\" = \"#cc0000\",\r\n    \"Scotland\" = \"#00004d\",\r\n    \"Italy\" = \"#1a1aff\"\r\n  )\r\n  \r\n  text_color <- switch(i,\r\n    \"Ireland\" = \"white\",\r\n    \"France\" = \"white\",\r\n    \"England\" = \"red\",\r\n    \"Wales\" = \"white\",\r\n    \"Scotland\" = \"white\",\r\n    \"Italy\" = \"white\"\r\n  )\r\n  \r\n  plots[[i]] <-\r\n    all_results |>\r\n    filter(country == i) |>\r\n    mutate(after = as.numeric(year >= 2017)) |>\r\n    rename(tries_per_year = TF) |>\r\n    mutate(\r\n      mean = mean(tries_per_year), .by = after\r\n    ) |>\r\n    mutate(\r\n      mean_before = ifelse(after == 0, mean, NA),\r\n      mean_after = ifelse(after == 1, mean, NA)\r\n    ) |>\r\n    ggplot(aes(year, tries_per_year)) +\r\n    geom_point(\r\n      color = \"black\", \r\n      fill = ifelse(main_color == \"white\", text_color, main_color), \r\n      alpha = 0.3, \r\n      shape = 21, \r\n      size = 2.5\r\n    ) +\r\n    geom_line(\r\n      aes(y = mean_before), \r\n      linetype = \"longdash\", \r\n      color = ifelse(main_color == \"white\", text_color, main_color), \r\n      linewidth = 0.8\r\n    ) +\r\n    geom_line(\r\n      aes(y = mean_after), \r\n      linetype = \"longdash\", \r\n      color = ifelse(main_color == \"white\", text_color, main_color), \r\n      linewidth = 0.8\r\n    ) +\r\n    geom_vline(xintercept = 2016.5, linetype = \"dotted\") +\r\n    ylim(c(0, 30)) +\r\n    labs(\r\n      x = labs$x,\r\n      y = labs$y\r\n    ) +\r\n    facet_grid(. ~ country) +\r\n    theme_custom() +\r\n    theme(\r\n      strip.background = element_rect(fill = main_color, color = \"black\"),\r\n      strip.text = element_text(size = 36, colour = text_color),\r\n      axis.title = element_text(size = 28),\r\n      axis.text = element_text(size = 25)\r\n    )\r\n}\r\n\r\nwrap_plots(plots, ncol = 2)\r\n\r\n\r\n\r\n\r\n\r\nThe dashed lines before and after 2017 show the average number of tries per\r\ncountry and per tournament. We can see that, on average, all teams scored more\r\ntries after 2017 than before 2017.\r\nHowever, this change is very heterogenous: some countries had a large increase\r\n(Ireland, Scotland), some had a moderate change (France, England), and other\r\ndidn’t seem to be affected a lot (Wales, Italy).\r\n\r\nThis is to ensure that a team that makes the Grand\r\nSlam also wins the tournament.↩︎\r\n",
    "preview": "posts/2023-03-01-do-bonus-points-lead-to-more-tries-in-6-nations-matches/distill-preview.png",
    "last_modified": "2023-03-13T14:33:13+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2023-01-14-where-should-my-family-meet/",
    "title": "Where should my family meet?",
    "description": "Or a mathematical way to evade the debate on the location of the next holidays.",
    "author": [
      {
        "name": "Etienne Bacher",
        "url": {}
      }
    ],
    "date": "2023-01-14",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nThe problem on a 2D plan\r\nThe problem with a sphere\r\nFinding points on a sphere\r\nComputing the distance\r\nMaking an interactive globe\r\n\r\n\r\nLike many others, my family is quite split geographically: some are in Europe but\r\nin different countries, and some are in other continents. Given this distance\r\nbetween us, we don’t gather in a single place that often.\r\nThis got me thinking: in which place should we meet if we wanted to minimize the\r\ntotal distance, i.e the sum of the distances made by each one? I’m just thinking\r\nin terms of distance as the crow flies, because of course the distance also depends\r\non where we must go to take the plane, on the potential flight connections we have\r\nto make, etc.\r\nThis question is frequent in a lot of optimal location problems. For example,\r\nwhere should a factory be built so that it minimizes the sum of distances to a\r\nlist of warehouses? However, I didn’t know about it before thinking about my\r\noriginal question, and it’s not as simple as it looked like to me. I thought a\r\nbit more about that, searched online, asked to other members of my family more\r\ncomfortable with maths, and this blog post summarizes what I learnt from that.\r\nAs every other post on this blog, it will include some R code, but that will\r\ncome later.\r\nThe problem on a 2D plan\r\nThe question I want to answer is: where should my family meet in order to minimize\r\nthe total distance? Something that is not clearly mentioned here is that I want\r\nto minimize the total distance on a sphere (because, surprise, the Earth is\r\nclose to a sphere). This adds a layer of complexity, so let’s start with a 2D\r\nanalysis.\r\nSuppose I have 5 points placed on a grid like below:\r\n\r\n\r\n\r\n\r\n\r\nI want to find the point \\((X,Y)\\) that minimizes the sum of distances from each\r\npoint to \\((X,Y)\\). Here, I use the Euclidean distance, but other measures\r\nare possible (such as the Manhattan distance). The formula for the Euclidean distance between two points \\((x_1, y_1)\\) and \\((x_2, y_2)\\) is:\r\n\\[dist = \\sqrt{(x_1-x_2)^2 + (y_1-y_2)^2}\\]\r\nFor example, the distance between the points \\((1,0)\\) and \\((2,1)\\) is1:\r\n\\[\\begin{align}\r\ndist = & \\sqrt{(1-2)^2 + (0-1)^2}\\\\\r\n= & \\sqrt{1+1} \\\\\r\n= & \\sqrt{2}\r\n\\end{align}\\]\r\nHere, I want to find the point \\((X,Y)\\) that minimizes:\r\n\\[Total \\ distance = D = \\sum_{i=1}^5\\sqrt{(x_i-X)^2 + (y_i-Y)^2}\\]\r\nThe point \\((X,Y)\\) that solves this is called the geometric median, or \\(L_1\\)-median. If we\r\nonly had two points, then any point on the segment between those two points would\r\nbe a solution2, but here, we have five points.\r\nHowever, according to Wikipedia,\r\n\r\nDespite the geometric median’s being an easy-to-understand concept, computing it\r\nposes a challenge. […] Therefore, only numerical or symbolic approximations to\r\nthe solution of this problem are possible under this model of computation.\r\n\r\nIn other words, while it is theoretically possible to compute the exact solution\r\nto this problem, it is impossible to do so in reasonable time in practice when\r\nthe number of points is very large (note that there are some special cases, such\r\nas \\(n=3\\) or \\(n=4\\)). This is why we need to use an approximation algorithm.\r\nWe can use the Nelder-Mead method, which is a common method for function\r\nminimization. We first take a starting point, say \\((0, 0)\\). Two other points\r\nwill be taken randomly. Then, the algorithm computes the function we want to\r\nminimize (here, the total distance) for each of the three random points. The two\r\nlowest points are kept, and the algorithm replaces the third one by its symmetric\r\npoint relative to the line between the two lowest points. But an animation is\r\nworth a thousand words:\r\n\r\n Animation by Nicoguaro - Own work, CC BY 4.0, https://commons.wikimedia.org/w/index.php?curid=51597575 \r\nIn the animation above, the triangle moves and shrinks until it reaches the\r\nminimum. There are more available options than just reflecting the highest point.\r\nI found these two blog posts very helpful to understand how the Nelder-Mead\r\nmethod works:\r\nby Mathias Brandewinder\r\nby Alex Dowad\r\nAnother algorithm that is commonly used for that is the Weiszfeld algorithm. The\r\nidea is to start from a point \\((X_0, Y_0)\\), update it using its derivatives to\r\nget \\((X_1, Y_1)\\), and continue this process until the distance between two updates\r\nis under a certain threshold. I won’t use this method here, so click on the arrow\r\nbelow if you want more details.\r\n\r\nClick here to have more details about Weiszfeld algorithm and its R implementation.\r\n\r\nList of steps in Weiszfeld algorithm:\r\npick a random point \\(P_0 = (X_0, Y_0)\\)\r\ncompute \\(X_1 = \\frac{\\sum_{i=1}^5 \\frac{x_i}{\\sqrt{(x_i-X_0)^2 + (y_i-Y_0)^2}}}{\\sum_{i=1}^5 \\frac{1}{\\sqrt{(x_i-X_0)^2 + (y_i-Y_0)^2}}}\\) and \\(Y_1 = \\frac{\\sum_{i=1}^5 \\frac{x_i}{\\sqrt{(x_i-X_0)^2 + (y_i-Y_0)^2}}}{\\sum_{i=1}^5 \\frac{1}{\\sqrt{(x_i-X_0)^2 + (y_i-Y_0)^2}}}\\)\r\ncompute the distance \\(\\varepsilon\\) between \\((X_0, Y_0)\\) and \\((X_1, Y_1)\\)\r\nrepeat steps 2 and 3 until \\(\\varepsilon\\) is lower than an arbitrary threshold. This will give an approximate solution \\((X, Y)\\).\r\nTo get the expressions above, we differentiate with respect to \\(X\\):\r\n\\[\\begin{equation}\r\n\\frac{\\partial D}{\\partial X} = 0 \\\\\r\n\\sum_{i=1}^5\\frac{\\partial \\sqrt{(x_i-X)^2 + (y_i-Y)^2}}{\\partial X} = 0 \\\\\r\n\\sum_{i=1}^5 -\\frac{- 2x_i + 2X}{2\\sqrt{(x_i-X)^2 + (y_i-Y)^2}} = 0 \\\\\r\n\\sum_{i=1}^5 \\frac{x_i - X}{\\sqrt{(x_i-X)^2 + (y_i-Y)^2}} = 0 \\\\\r\n\\sum_{i=1}^5 \\frac{x_i}{\\sqrt{(x_i-X)^2 + (y_i-Y)^2}} - \\sum_{i=1}^5 \\frac{X}{\\sqrt{(x_i-X)^2 + (y_i-Y)^2}} = 0 \\\\\r\n\\sum_{i=1}^5 \\frac{x_i}{\\sqrt{(x_i-X)^2 + (y_i-Y)^2}} - X \\sum_{i=1}^5 \\frac{1}{\\sqrt{(x_i-X)^2 + (y_i-Y)^2}} = 0 \\\\\r\nX = \\ \\frac{\\sum_{i=1}^5 \\frac{x_i}{\\sqrt{(x_i-X)^2 + (y_i-Y)^2}}}{\\sum_{i=1}^5 \\frac{1}{\\sqrt{(x_i-X)^2 + (y_i-Y)^2}}} \\\\\r\nX^* = \\ T(X^*)\r\n\\end{equation}\\]\r\nSimilarly,\r\n\\[\\begin{align}\r\nY^* = & \\ \\frac{\\sum_{i=1}^5 \\frac{x_i}{\\sqrt{(x_i-X)^2 + (y_i-Y^*)^2}}}{\\sum_{i=1}^5 \\frac{1}{\\sqrt{(x_i-X)^2 + (y_i-Y^*)^2}}} \\\\\r\nY^* = & \\ T(Y^*)\r\n\\end{align}\\]\r\nNow, we can make a loop like the following:\r\nstart with \\(X = X_0\\) and \\(Y = Y_0\\), and compute \\(X_1 = T(X_0)\\) and \\(Y_1 = T(Y_0)\\)\r\ncompute \\(X_2 = T(X_1)\\) and \\(Y_2 = T(Y_1)\\)\r\ncontinue until the distance between \\((X_n, X_{n+1})\\) and \\((Y_n, Y_{n+1})\\) is smaller than an arbitrary \\(\\varepsilon\\).\r\nOnce again, I’m not mathematician, so this may seem not rigorous at all for someone with more experience. If you’re interested in a rigorous explanation of Weiszfeld’s algorithm, check out this paper (but there are many others online).\r\n\r\nAs usual with R, when you think of a widely used algorithm or feature, there’s necessarily an R package for that. Here, I will use the package Gmedian and the function Weiszfeld():\r\n\r\n\r\nlibrary(Gmedian)\r\n\r\n\r\nThis function has 4 arguments:\r\nX is a matrix of points, where each row is an observation;\r\nweights is useful if we want to give more importance to some points. Here, we assume that all 4 points are equally important, so we set it to NULL (the default);\r\nepsilon is the threshold below which the algorithm will stop;\r\nnitermax is the maximum number of iterations that will be run. This is complementary to epsilon: the algorithm stops as soon as the difference between two \\((X, Y)\\) is lower than epsilon or as the algorithm hits the maximum number of iterations.\r\nWe can keep the defaults for epsilon and nitermax, so we just need to create a matrix containing our four points, and run this in Weiszfeld():\r\n\r\n\r\n# Create matrix\r\nmy_points <- rbind(c(1, 0), c(2, 1), c(-3, 0), c(0, 3), c(-2,2))\r\nmy_points\r\n\r\n     [,1] [,2]\r\n[1,]    1    0\r\n[2,]    2    1\r\n[3,]   -3    0\r\n[4,]    0    3\r\n[5,]   -2    2\r\n\r\nmedian_point <- Weiszfeld(my_points)\r\nmedian_point\r\n\r\n$median\r\n           [,1]    [,2]\r\n[1,] -0.2704185 1.36027\r\n\r\n$iter\r\n[1] 33\r\n\r\nWe can now compute the sum of distances between each original point and the geometric median:\r\n\r\n\r\nlist_dist <- c()\r\nfor (i in 1:nrow(my_points)) {\r\n  foo <- my_points[i, ]\r\n  list_dist[i] <- dist(rbind(foo, median_point$median))\r\n}\r\nsum(list_dist)\r\n\r\n[1] 10.71581\r\n\r\nWe can use the Nelder-Mead algorithm in R with the function optim() in the\r\nstats package (included in base R). First, we write the objective function and\r\nfeed optim() with it, along with the parameters (our list of points and a point\r\nfrom which to start).\r\n\r\n\r\n# Inputs:\r\n# - starting_p: a vector (x, y) indicating from which point to start\r\n# - my_p: a matrix where each row is a point in our list\r\ncriterion_2D <- function(starting_p, my_p) {\r\n  # Formula for the sum of Euclidean distances\r\n  f <- sum(sqrt((starting_p[1] - my_p[, 1])^2 + (starting_p[2] - my_p[, 2])^2))\r\n}\r\n\r\noutput <- optim(par = c(0, 0), criterion_2D, my_p = my_points)\r\n\r\n# Location of the optimal point\r\noutput$par\r\n\r\n[1] -0.2701872  1.3599835\r\n\r\n# Total distance\r\noutput$value\r\n\r\n[1] 10.71581\r\n\r\nAs we can see, this solution automatically gives us the optimal location and the\r\ntotal distance. It also doesn’t require an external package, which is interesting\r\nif you want to reduce the dependencies you use. In the example above, the optimal point is therefore at (-0.27, 1.36),\r\nand the total distance is 10.72:\r\n\r\n\r\n\r\nNow that we know how to solve the problem in 2D, let’s move to 3D with a sphere,\r\nwhere it is slightly more complicated.\r\nThe problem with a sphere\r\nFinding points on a sphere\r\nPoints on a sphere are often referred to by their latitude and longitude. However,\r\nif we want to compute the distance between points on a sphere, we need to get\r\n3 coordinates \\((x,y,z)\\). How do we do that?\r\nFirst, we have to change the unit of the points to use radians instead of degrees.\r\nThis is done by multiplying the values in degrees by pi and dividing them by 180.\r\nThen, we need to compute the 3 coordinates \\(x\\), \\(y\\), and \\(z\\) as follows:\r\n\\(x = cos(latitude) \\times cos(longitude) \\times R\\)\r\n\\(y = cos(latitude) \\times sin(longitude) \\times R\\)\r\n\\(z = sin(latitude) \\times R\\)\r\nwhere \\(R\\) is the radius of the sphere.\r\nLet’s make an example. We define some random points on a sphere with their\r\nlatitude and longitude in degrees:\r\n\r\n\r\n# R = earth radius (km) \r\nR <- 6200\r\n\r\n# Latitude, longitude for a few locations in degrees\r\nlatitude <- c(45, -40, 30, -30)\r\nlongitude <- c(-10, 10, 50, 50)\r\n\r\n# Convert to radians\r\nlatitude_r <- latitude * pi / 180\r\nlongitude_r <- longitude * pi / 180\r\n\r\n# x,y,z coordinates for the locations\r\nx <- cos(latitude_r) * cos(longitude_r) * R\r\ny <- cos(latitude_r) * sin(longitude_r) * R\r\nz <- sin(latitude_r) * R\r\n\r\nmy_points <- cbind(x,y,z)\r\nmy_points\r\n\r\n            x         y         z\r\n[1,] 4317.458 -761.2844  4384.062\r\n[2,] 4677.320  824.7378 -3985.283\r\n[3,] 3451.356 4113.1665  3100.000\r\n[4,] 3451.356 4113.1665 -3100.000\r\n\r\nComputing the distance\r\nWe know how to express the location of points using three coordinates. We can now\r\nthink about how we will measure the distance between these points.\r\nSuppose we have two points, \\(P_1\\) and \\(P_2\\), and we want to measure the distance\r\n\\(l\\). If we were in an Euclidean space, we would compute the distance \\(d\\) between\r\nthe two points, which is equal to \\(\\sqrt{(x_1-x_2)^2 + (y_1-y_2)^2 + (z_1-z_2)^2}\\),\r\nbut that’s not what we’re looking for because it doesn’t take into account the\r\ncurvature of the sphere.\r\n\r\n\r\n\r\nBy definition, \\(l = R \\times \\theta\\). We know the radius, so we need to compute\r\n\\(\\theta\\). The triangle is isosceles, so dividing the angle in two equal parts\r\nwill give us two rectangle triangles where we can compute \\(\\frac{\\theta}{2}\\).\r\nIndeed,\r\n\\[sin(\\frac{\\theta}{2}) = \\frac{d/2}{R} \\]\r\n\\[\\frac{\\theta}{2} = arcsin(\\frac{d/2}{R})\\]\r\n\\[\\theta = 2 \\times arcsin(\\frac{d/2}{R})\\]\r\n\r\n\r\n\r\nTherefore, we have:\r\n\\[l = 2R \\times arcsin(\\frac{d/2}{R})\\]\r\nNow that we have a way to measure the distance between two points based on their\r\n3 coordinates, we can follow the same procedure as in the 2D case: make a\r\nfunction and give it to optim(). However, the objective function to minimize\r\nis different because we now use the formula above for the distance.\r\n\r\n\r\n# Inputs:\r\n# - starting_p: a vector (lat, long, both in degrees) indicating from which point to start\r\n# - my_p: a matrix where each row is a point in our list, and 3 columns (one for\r\n#   each dimension)\r\ncriterion_3D <- function(starting_p, my_p) {\r\n  # Convert degrees in radians\r\n  plat <- starting_p[1] * pi / 180\r\n  plon <- starting_p[2] * pi / 180\r\n  # Compute the x, y, z coordinates\r\n  x <- cos(plat) * cos(plon) * R\r\n  y <- cos(plat) * sin(plon) * R\r\n  z <- sin(plat) * R\r\n\r\n  # Return the total distance\r\n  sum(\r\n    2*R*asin(\r\n      sqrt(\r\n        (x - my_points[, 1])^2 + (y - my_points[, 2])^2 + (z - my_points[, 3])^2\r\n      )\r\n    / 2 /R)\r\n  )\r\n}\r\n\r\n\r\nWe can now apply once again the optim() function:\r\n\r\n\r\n# Initial point (latitude, longitude, in degrees)\r\ny <- optim(c(0,0), criterion_3D, my_p = my_points) \r\ny$par\r\n\r\n[1] -4.532362 31.660719\r\n\r\ny$value\r\n\r\n[1] 18606.59\r\n\r\nSo in this dummy example, the optimal location is in -4.53°\r\nlat., 31.66° long., and the total distance\r\n18606.59 km.\r\nMaking an interactive globe\r\nNow the most important part: show the solution on a globe! There are several ways\r\nto do this. One of them is to use echarts4r:\r\n\r\n\r\nlibrary(echarts4r)\r\nlibrary(echarts4r.assets)\r\ncoords <- data.frame(\r\n  lat = latitude,\r\n  long = longitude,\r\n  lat_sol = y$par[1],\r\n  long_sol = y$par[2]\r\n)\r\n\r\ncoords |> \r\n  e_charts() |> \r\n  \r\n  # create the globe\r\n  e_globe(\r\n    base_texture = ea_asset(\"world\"), \r\n    displacementScale = 0.05,\r\n    shading = \"color\",\r\n    viewControl = list(autoRotate = FALSE, targetCoord = c(10, 0))\r\n  ) |> \r\n  \r\n  # add the starting points\r\n  e_scatter_3d(\r\n    long, lat,\r\n    coord_system = \"globe\",\r\n    symbolSize = 15,\r\n    itemStyle = list(color = \"red\"),\r\n    emphasis = list(label = list(show = FALSE))\r\n  ) |> \r\n  \r\n  # add the solution\r\n  e_scatter_3d(\r\n    long_sol, lat_sol,\r\n    coord_system = \"globe\",\r\n    symbolSize = 15,\r\n    itemStyle = list(color = \"yellow\")\r\n  ) |> \r\n  \r\n  # add tooltip with latitude and longitude (only works for\r\n  # starting points)\r\n  e_tooltip(\r\n    trigger = \"item\",\r\n    formatter = htmlwidgets::JS(\"\r\n      function(params){\r\n        return('Longitude: ' + params.value[0] + '<br />Latitude: ' + params.value[1])\r\n      }\r\n    \")\r\n  ) |> \r\n  e_legend(FALSE)\r\n\r\n\r\n\r\nWe now have a solution, but the question is: what if the optimal meeting point is\r\nlocated in the middle of the Pacific Ocean? That wouldn’t be the most convenient\r\npoint for a family meeting (unless you have a yacht).\r\nSo far, we didn’t care about this. We did some unconstrained optimization. The\r\nnext step is to add the constraint that the meeting cannot happen at a place covered by oceans. I will try to explore that in a future post, but so far I\r\ndidn’t find many resources on this. If you have some ideas on how to do this or\r\nwhere to start from, feel free to let a comment.\r\nThanks for having read so far!\r\n\r\nTo be sure,\r\nwe know that the distance between those two points is the diagonal of a square\r\nwith sides of length 1, and that the length of the diagonal of a square with\r\nsides of length \\(a\\) is \\(a\\sqrt{2}\\).↩︎\r\nFor example, if the two points are separated by 1000 km, then\r\nputting the meeting point at 200 km from one and 800 km from the other would give\r\nthe same total distance as putting the meeting point at 500 km far from each\r\npoint.↩︎\r\n",
    "preview": "posts/2023-01-14-where-should-my-family-meet/distill-preview.png",
    "last_modified": "2023-03-13T14:33:13+01:00",
    "input_file": {},
    "preview_width": 1382,
    "preview_height": 1076
  },
  {
    "path": "posts/2022-11-28-some-notes-about-improving-base-r-code/",
    "title": "Some notes about improving base R code",
    "description": "A small collection of tips to make base R code faster.",
    "author": [
      {
        "name": "Etienne Bacher",
        "url": {}
      }
    ],
    "date": "2022-11-28",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nCheck if a vector has a single value\r\nConcatenate columns\r\nGiving attributes to large dataframes\r\nFind empty rows\r\nConclusion\r\n\r\nPreview image coming from: https://trainingindustry.com/magazine/nov-dec-2018/life-in-the-fast-lane-accelerated-continuous-development-for-fast-paced-organizations/\r\nLately I’ve spent quite some time on packages that require (almost) only base R:\r\ndatawizard, a package belonging to the easystats ecosystem, whose goal is to\r\nprovide tools for data wrangling and statistical transformations;\r\npoorman, whose goal is to reproduce tidyverse functions (with a strong focus\r\non dplyr) using base R only.\r\nI’ve used bench::mark() and profvis::profvis() a lot to improve code performance\r\nand here are a few things I learnt. By default, bench::mark() checks that all\r\nexpressions return the same output, so we can be confident that the alternatives\r\nI show in this post are truly equivalent.\r\nBefore we start, I want to precise a few things.\r\nFirst, these performance improvements are targeted to package developers. A\r\nrandom user shouldn’t really care if a function takes 200 milliseconds less to\r\nrun. However, I think a package developer might find these tips interesting.\r\nSecond, if you find some ways to speed up my alternatives, feel free to comment.\r\nI know that there are a bunch of packages whose reputation is built on being very\r\nfast (for example data.table and collapse). I’m only showing some base R\r\ncode alternatives here.\r\nFinally, here’s a small function that I use to make a classic dataset\r\n(like iris or mtcars) much bigger.\r\n\r\n\r\nmake_big <- function(data, nrep = 500000) {\r\n  tmp <- vector(\"list\", length = nrep)\r\n  for (i in 1:nrep) {\r\n    tmp[[i]] <- data\r\n  }\r\n  \r\n  data.table::rbindlist(tmp) |> \r\n    as.data.frame()\r\n}\r\n\r\n\r\nCheck if a vector has a single value\r\nOne easy way to do this is to run length(unique(x)) == 1, which basically means\r\nthat first we have to collect all unique values and then count them. This can be\r\nquite inefficient: it would be enough to stop as soon as we find two different\r\nvalues.\r\nWhat we can do is to compare all values to the first value of the vector. Below is\r\nan example with a vector containing 10 million values. In the first case, it only\r\ncontains 1, and in the second case it contains 1 and 2.\r\n\r\n\r\n# Should be TRUE\r\ntest <- rep(1, 1e7)\r\n\r\nbench::mark(\r\n  length(unique(test)) == 1,\r\n  all(test == test[1]),\r\n  iterations = 10\r\n)\r\n\r\n# A tibble: 2 × 6\r\n  expression                  min  median `itr/sec` mem_alloc `gc/sec`\r\n  <bch:expr>              <bch:t> <bch:t>     <dbl> <bch:byt>    <dbl>\r\n1 length(unique(test)) =… 161.8ms 185.6ms      5.31   166.1MB     5.31\r\n2 all(test == test[1])     44.2ms  69.7ms     14.9     38.1MB     4.47\r\n\r\n# Should be FALSE\r\ntest2 <- rep(c(1, 2), 1e7)\r\n\r\nbench::mark(\r\n  length(unique(test2)) == 1,\r\n  all(test2 == test2[1]),\r\n  iterations = 10\r\n)\r\n\r\n# A tibble: 2 × 6\r\n  expression                  min  median `itr/sec` mem_alloc `gc/sec`\r\n  <bch:expr>              <bch:t> <bch:t>     <dbl> <bch:byt>    <dbl>\r\n1 length(unique(test2)) … 342.2ms 390.6ms      2.46   332.3MB     2.46\r\n2 all(test2 == test2[1])   63.2ms  71.6ms     11.5     76.3MB     2.30\r\n\r\nThis is also faster for character vectors:\r\n\r\n\r\n# Should be FALSE\r\ntest3 <- rep(c(\"a\", \"b\"), 1e7)\r\n\r\nbench::mark(\r\n  length(unique(test3)) == 1,\r\n  all(test3 == test3[1]),\r\n  iterations = 10\r\n)\r\n\r\n# A tibble: 2 × 6\r\n  expression                   min median `itr/sec` mem_alloc `gc/sec`\r\n  <bch:expr>               <bch:t> <bch:>     <dbl> <bch:byt>    <dbl>\r\n1 length(unique(test3)) =… 287.8ms  326ms      3.00   332.3MB     3.00\r\n2 all(test3 == test3[1])    82.7ms  107ms      8.73    76.3MB     1.75\r\n\r\nConcatenate columns\r\nSometimes we need to concatenate columns, for example if we want to create a\r\nunique id from several grouping columns.\r\n\r\n\r\ntest <- data.frame(\r\n  origin = c(\"A\", \"B\", \"C\"),\r\n  destination = c(\"Z\", \"Y\", \"X\"),\r\n  value = 1:3\r\n)\r\n\r\ntest <- make_big(test)\r\n\r\n\r\nOne option to do this is to combine paste() and apply() using MARGIN = 1 to\r\napply paste() to each row. However, a faster way to do this is to use do.call()\r\ninstead of apply():\r\n\r\n\r\nbench::mark(\r\n  apply = apply(test[, c(\"origin\", \"destination\")], 1, paste, collapse = \"_\"),\r\n  do.call = do.call(paste, c(test[, c(\"origin\", \"destination\")], sep = \"_\"))\r\n)\r\n\r\n# A tibble: 2 × 6\r\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\r\n  <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\r\n1 apply         7.36s    7.36s     0.136    80.1MB     5.71\r\n2 do.call    128.14ms 139.29ms     7.08     11.4MB     0   \r\n\r\nGiving attributes to large dataframes\r\nThis one comes from these StackOverflow question and answer. Manipulating a dataframe can remove some attributes. For example, if I give an\r\nattribute foo to a large dataframe:\r\n\r\n\r\norig <- data.frame(x1 = rep(1, 1e7), x2 = rep(2, 1e7))\r\nattr(orig, \"foo\") <- TRUE\r\nattr(orig, \"foo\")\r\n\r\n[1] TRUE\r\n\r\nIf I reorder the columns, this attribute disappears:\r\n\r\n\r\nnew <- orig[, c(2, 1)]\r\nattr(new, \"foo\")\r\n\r\nNULL\r\n\r\nWe can put it back with:\r\n\r\n\r\nattributes(new) <- utils::modifyList(attributes(orig), attributes(new))\r\nattr(new, \"foo\")\r\n\r\n[1] TRUE\r\n\r\nBut this takes some time because we also copy the 10M row names of the\r\ndataset. Therefore, one option is to create a custom function that only copies the\r\nattributes that were in orig but are not in new (in this case, only attribute\r\nfoo is concerned):\r\n\r\n\r\nreplace_attrs <- function(obj, new_attrs) {\r\n  for(nm in setdiff(names(new_attrs), names(attributes(data.frame())))) {\r\n    attr(obj, which = nm) <- new_attrs[[nm]]\r\n  }\r\n  return(obj)\r\n}\r\n\r\nbench::mark(\r\n  old = {\r\n    attributes(new) <- utils::modifyList(attributes(orig), attributes(new))\r\n    head(new)\r\n  },\r\n  new = {\r\n    new <- replace_attrs(new, attributes(orig))\r\n    head(new)\r\n  }\r\n)\r\n\r\n# A tibble: 2 × 6\r\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\r\n  <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\r\n1 old            68ms     82ms      12.9    38.2MB     4.29\r\n2 new          52.8µs   80.5µs   11188.     24.4KB     8.77\r\n\r\nFind empty rows\r\nIt can be useful to remove empty rows, meaning rows containing only NA or \"\".\r\nWe could once again use apply() with MARGIN = 1, but a faster way is to use\r\nrowSums(). First, we create a data frame full of TRUE/FALSE with is.na(test) | test == \"\",\r\nand then we count by row the number of TRUE. If this number is equal to the number\r\nof columns, then it means that the row only has NA or \"\".\r\n\r\n\r\ntest <- data.frame(\r\n  a = c(1, 2, 3, NA, 5),\r\n  b = c(\"\", NA, \"\", NA, \"\"),\r\n  c = c(NA, NA, NA, NA, NA),\r\n  d = c(1, NA, 3, NA, 5),\r\n  e = c(\"\", \"\", \"\", \"\", \"\"),\r\n  f = factor(c(\"\", \"\", \"\", \"\", \"\")),\r\n  g = factor(c(\"\", NA, \"\", NA, \"\")),\r\n  stringsAsFactors = FALSE\r\n)\r\n\r\ntest <- make_big(test, 100000)\r\n\r\nbench::mark(\r\n  apply = which(apply(test, 1, function(i) all(is.na(i) | i == \"\"))),\r\n  rowSums = which(rowSums((is.na(test) | test == \"\")) == ncol(test))\r\n)\r\n\r\n# A tibble: 2 × 6\r\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\r\n  <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\r\n1 apply         2.08s    2.08s     0.480   112.9MB     3.84\r\n2 rowSums    709.59ms 709.59ms     1.41     99.7MB     0   \r\n\r\nConclusion\r\nThese were just a few tips I discovered. Maybe there are ways to make them even\r\nfaster in base R? Or maybe you know some weird/hidden tips? If so, feel free to\r\ncomment below!\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-11-28-some-notes-about-improving-base-r-code/fast.jpeg",
    "last_modified": "2023-08-22T09:24:29+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-27-mapping-french-rivers-network/",
    "title": "Mapping French rivers network",
    "description": "",
    "author": [
      {
        "name": "Etienne Bacher",
        "url": {}
      }
    ],
    "date": "2021-12-27",
    "categories": [],
    "contents": "\r\nSpanish rivers network, by Dominic RoyéOnce again inspired by Dominic Royé’s maps, I decided to map rivers in France. The dataset I use comes from HydroSHEDS. The code below is quite similar to the code in my previous post so I don’t spend a lot of time on it.\r\n\r\n\r\nlibrary(ggplot2)\r\nlibrary(ggtext)\r\nlibrary(sf)\r\nlibrary(rnaturalearth)\r\n\r\n\r\n\r\n\r\n\r\nfrance <- ne_countries(country = \"France\", scale = 'medium',\r\n                       type = 'map_units', returnclass = 'sf')  \r\n\r\nrivers_30sec <- read_sf(\"eu_riv_30s/eu_riv_30s.shp\") |>\r\n  st_intersection(france)\r\n\r\n\r\n\r\n\r\n\r\nx <- ggplot() +\r\n  geom_sf(\r\n    data = rivers_30sec, \r\n    color = \"#002266\"\r\n  ) +\r\n  labs(\r\n    title = \"Rivers network in France\",\r\n    subtitle = \"This map displays 18,099 rivers. These are<br> measured at a grid resolution of 30 arc-seconds<br> (approx. 1km at the equator).\",\r\n    caption = \"Made by Etienne Bacher &middot; Data from HydroSHEDS\"\r\n  ) +\r\n  theme_void() +\r\n  theme(\r\n    plot.background = element_rect(fill = \"white\", color = \"white\"),\r\n    panel.background = element_rect(fill = \"white\", color = \"white\"),\r\n    plot.title = element_markdown(hjust = 0.5, size = 30, margin = margin(t = 10, b = -20)),\r\n    plot.subtitle = element_markdown(margin = margin(t = 40, b = -60, l = 10), size = 12),\r\n    plot.caption = element_markdown(hjust = 0.5, margin = margin(l = 10, b = 20, t = -30)),\r\n    text = element_text(family = \"Roboto Condensed\")\r\n  )\r\n\r\nggsave(\"france_30sec.png\", plot = x, width = 8, height = 8)\r\n\r\n\r\n\r\n\r\nThis plot shows the density of rivers in France. Now, if we want to show which rivers are the most important, we can modify the opacity of the lines depending on their flow:\r\n\r\n\r\nx <- ggplot() +\r\n  geom_sf(\r\n    data = rivers_30sec, \r\n    mapping = aes(alpha = UP_CELLS),\r\n    color = \"#002266\"\r\n  ) +\r\n  labs(\r\n    title = \"Rivers network in France\",\r\n    subtitle = \"This map displays 18,099 rivers. These are<br> measured at a grid resolution of 30 arc-seconds<br> (approx. 1km at the equator). Line opacity<br> represents the size of the flow.\",\r\n    caption = \"Made by Etienne Bacher &middot; Data from HydroSHEDS\"\r\n  ) +\r\n  theme_void() +\r\n  theme(\r\n    plot.background = element_rect(fill = \"white\", color = \"white\"),\r\n    panel.background = element_rect(fill = \"white\", color = \"white\"),\r\n    plot.title = element_markdown(hjust = 0.5, size = 30, margin = margin(t = 10, b = -20)),\r\n    plot.subtitle = element_markdown(margin = margin(t = 40, b = -60, l = 10), size = 12),\r\n    legend.position = \"none\",\r\n    plot.caption = element_markdown(hjust = 0.5, margin = margin(l = 10, b = 20, t = -30)),\r\n    text = element_text(family = \"Roboto Condensed\")\r\n  )\r\n\r\nggsave(\"france_30sec_opac.png\", plot = x, height = 8, width = 8)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-27-mapping-french-rivers-network/france_30sec_opac.png",
    "last_modified": "2022-05-03T17:54:55+02:00",
    "input_file": {},
    "preview_width": 2400,
    "preview_height": 2400
  },
  {
    "path": "posts/2021-12-23-reproduce-some-maps-about-3g-and-4g-access/",
    "title": "Reproduce some maps about 3G and 4G access",
    "description": "I try to reproduce some maps made by Dominic Royé.",
    "author": [
      {
        "name": "Etienne Bacher",
        "url": {}
      }
    ],
    "date": "2021-12-23",
    "categories": [],
    "contents": "\r\nA few weeks ago, I found out about some maps made by Dominic Royé (see his gallery for more graphs). Among all those beautiful graphs, I decided to try reproducing those on 3G and 4G cell towers redistribution, like the one below for France.\r\nThis is the original plot, by Dominic Royé.Note that what I do here is just reproducing some maps to train with spatial data. All credits for the original maps go to Dominic Royé.\r\nBefore we start, these are the packages we will need:\r\n\r\n\r\nlibrary(dplyr)\r\nlibrary(readr)\r\nlibrary(ggplot2)\r\nlibrary(ggtext)\r\nlibrary(sf)\r\nlibrary(rnaturalearth)\r\n\r\n\r\n\r\nNote that I will use native pipes |>, that are only available with R 4.1.0 and later versions. If your version is older, you can simply replace those by %>% from the package magrittr.\r\nObtain and treat the data\r\nObtain the data\r\nAs mentioned in the original plot, the data on cell towers location come from OpenCellid, which is a community-created dataset containing the location of cell towers for several radio signals: LTE, UMTS and GSM.\r\nThis data is free to obtain, we only need to register with an email address and to precise the general purpose for which we want to obtain the data. We can then choose the country and download the data in a CSV file. Note that this dataset is updated daily, so the map could be a bit different if you do it much later.\r\nTreat the data\r\nThe data treatment is actually very simple because the data is already very tidy. The only thing to do here is to add geographical attributes to the dataframe with st_as_sf():\r\n\r\n\r\ncelltowers <- read_csv(\"208.csv\") |> \r\n  st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4326)\r\n\r\n\r\n\r\nWhile playing with the data, I noticed that the dates of creation of cell towers are expressed in seconds (more precisely in UNIX timestamp), so if we want to convert these as more natural dates, we can use lubridate::as_datetime(). I didn’t use the dates to make the maps but it’s still useful to know how to do that.\r\nPlot the data\r\nWe have three things to do:\r\nplot an empty map of France;\r\nadd 3G cell towers location;\r\nadd 4G cell towers location;\r\nPlot an empty map of France\r\nTo do that, we can use the package rnaturalearth:\r\n\r\n\r\nfrance <- ne_countries(scale = 'medium', type = 'map_units', returnclass = 'sf') |>\r\n  filter(name == 'France') \r\n\r\n\r\n\r\nWe can already plot that:\r\n\r\n\r\nggplot() + \r\n  geom_sf(data = france)\r\n\r\n\r\n\r\n\r\nAdd 3G and 4G cell towers location\r\nWhile plotting those cell towers location, I noticed that, for some reason, some were located outside of France. Therefore, I chose keep only the locations that are inside the map of France we have. This can be done with sf::st_intersection(). Following Dominic Royé, I also separate 3G (UMTS) and 4G (LTE) cell towers (this step is time consuming):\r\n\r\n\r\nUMTS <- celltowers |> \r\n  filter(radio == \"UMTS\") |>\r\n  st_intersection(france)\r\n\r\nLTE <- celltowers |> \r\n  filter(radio == \"LTE\") |>\r\n  st_intersection(france)\r\n\r\n\r\n\r\nNow, we could plot all those points, but there are so many that it would completely hide the map so we need to use the smallest size possible for each point. Contrary to what one might expect, this is not done with the size argument but with the shape argument. Indeed, setting shape = \".\" means that each point will be the size of one pixel1. Still there are too many points compared to the original plot. The solution here is to decrease the opacity of each point:\r\n\r\n\r\nggplot() + \r\n  geom_sf(data = france, fill = \"black\", color = \"white\", size = 0.3) + \r\n  geom_sf(data = UMTS, shape = \".\", color = \"#4d88ff\", alpha = 0.3) +\r\n  geom_sf(data = LTE, shape = \".\", color = \"#cc0000\", alpha = 0.5)\r\n\r\n\r\n\r\n\r\nNow, we simply have to customize the plot a bit and to add labels, and we’re done! Here is the final plot (this is time consuming because of the large number of points):\r\n\r\n\r\nx <- ggplot() + \r\n  geom_sf(data = france, fill = \"black\", color = \"white\", size = 0.3) + \r\n  geom_sf(data = UMTS, shape = \".\", color = \"#4d88ff\", alpha = 0.3) +\r\n  geom_sf(data = LTE, shape = \".\", color = \"#cc0000\", alpha = 0.5) +\r\n  theme_void() +\r\n  labs(\r\n    title = \"<span style='color: white; text-align: center;'>Cell tower distribution with <br> <span style='color: #cc0000'>4G<\/span> (LTE) and <span style='color: #4d88ff'>3G<\/span> (UMTS)<\/span>\",\r\n    caption = \"<span style='color: white;'> Made by Etienne Bacher <b>&middot;<\/b> Original plot by Dominic Royé <b>&middot;<\/b> Data from opencellid.org <\/span>\"\r\n  ) +\r\n  theme(\r\n    plot.background = element_rect(fill = \"black\"),\r\n    panel.background = element_rect(fill = \"black\"),\r\n    plot.title = element_markdown(margin = margin(t = 40, b = -60, l = 10), size = 20),\r\n    plot.caption = element_markdown(hjust = 0, margin = margin(l = 10, b = 20, t = -30)),\r\n    text = element_text(family = \"Roboto Condensed\")\r\n  )\r\n\r\nggsave(\"my_plot_fr.png\", plot = x, height = 7, width = 7)\r\n\r\n\r\n\r\nMy reproduction of the plotIf you want to do it for another country than France, such as England, you can simply download the data for England on opencellid.org, use the map of England from rnaturalearth, apply the same code as above, and voilà!\r\n\r\n\r\nShow code\r\n\r\ncelltowers_eng <- read_csv(\"234.csv\") |> \r\n  st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4326)\r\n\r\neng <- ne_countries(scale = 'medium', type = 'map_units', returnclass = 'sf') |>\r\n  filter(name == 'England') \r\n\r\nUMTS <- celltowers_eng |> \r\n  filter(radio == \"UMTS\") |>\r\n  st_intersection(eng)\r\n\r\nLTE <- celltowers_eng |> \r\n  filter(radio == \"LTE\") |>\r\n  st_intersection(eng)\r\n\r\nx <- ggplot() + \r\n  geom_sf(data = eng, fill = \"black\", color = \"white\", size = 0.3) + \r\n  geom_sf(data = UMTS, shape = \".\", color = \"#4d88ff\", alpha = 0.3) +\r\n  geom_sf(data = LTE, shape = \".\", color = \"#cc0000\", alpha = 0.5) +\r\n  theme_void() +\r\n  labs(\r\n    title = \"<span style='color: white; text-align: center;'>Cell tower distribution with <br> <span style='color: #cc0000'>4G<\/span> (LTE) and <span style='color: #4d88ff'>3G<\/span> (UMTS)<\/span>\",\r\n    caption = \"<span style='color: white;'> Made by Etienne Bacher <b>&middot;<\/b> Original plot by Dominic Royé <b>&middot;<\/b> Data from opencellid.org <\/span>\"\r\n  ) +\r\n  theme(\r\n    plot.background = element_rect(fill = \"black\"),\r\n    panel.background = element_rect(fill = \"black\"),\r\n    plot.title = element_markdown(margin = margin(t = 40, b = -60, l = 10), size = 20),\r\n    plot.caption = element_markdown(hjust = 0, margin = margin(l = 10, b = 20, t = -30)),\r\n    text = element_text(family = \"Roboto Condensed\")\r\n  )\r\n\r\nggsave(\"my_plot_eng.png\", plot = x, height = 10, width = 9)\r\n\r\n\r\n\r\n\r\n\r\nThanks to this StackOverflow answer for pointing that out.↩︎\r\n",
    "preview": "posts/2021-12-23-reproduce-some-maps-about-3g-and-4g-access/my_plot_fr.png",
    "last_modified": "2022-05-03T17:54:55+02:00",
    "input_file": {},
    "preview_width": 2100,
    "preview_height": 2100
  },
  {
    "path": "posts/2021-04-11-how-to-create-a-gallery-in-distill/",
    "title": "How to create a gallery in Distill",
    "description": "Use lightgallery.js to create a gallery for your plots or images.",
    "author": [
      {
        "name": "Etienne Bacher",
        "url": {}
      }
    ],
    "date": "2021-05-20",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nCreate a gallery with lightgallery.js\r\nMake the gallery with R\r\nCreate thumbnails\r\nBuild the HTML structure\r\n\r\nUpdate GitHub Actions\r\nBonus: make a gallery for #tidytuesday\r\nConclusion\r\n\r\nThis post shows how to create a gallery on a Distill website. Keep in mind that Distill is (purposely) less flexible than other tools, such as {blogdown}, so the gallery might look quite different from what you expect.\r\nCreate a gallery with lightgallery.js\r\nLightgallery.js is a Javascript library that allows you to build a gallery very simply. You will need images in full size and thumbnails, i.e a smaller version of the images (we will see how to automatically make them later in this post).\r\nFirst of all, let’s construct the gallery with HTML, CSS, and Javascript. We will see how to adapt this in R then. We need to load the Javascript and CSS files for lightgallery.js in the head:\r\n\r\n<head>\r\n\r\n<link type=\"text/css\" rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.0/css/lightgallery.min.css\" /> \r\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/lightgallery-js/1.4.1-beta.0/js/lightgallery.min.js\"><\/script>\r\n\r\n<!-- lightgallery plugins -->\r\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/lg-fullscreen/1.2.1/lg-fullscreen.min.js\"><\/script>\r\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/lg-thumbnail/1.2.1/lg-thumbnail.min.js\"><\/script>\r\n\r\n<\/head>\r\n\r\nThen, we construct the layout of the gallery. Here, I make the minimum layout, just to make sure this works:\r\n\r\n<div id=\"lightgallery\">\r\n  <a href=\"img1.png\">\r\n    <img src=\"thumb-img1.png\" />\r\n  <\/a>\r\n  <a href=\"img2.png\">\r\n    <img src=\"thumb-img2.png\" />\r\n  <\/a>\r\n<\/div>\r\n\r\nAs you can see, the whole gallery is in a <div> element. To add an image to the gallery, we just have to add an <a> element as the two already there.\r\nThen, we add the Javascript code to run lightgallery.js:\r\n\r\n<script type=\"text/javascript\">\r\n  lightGallery(document.getElementById('lightgallery')); \r\n<\/script>\r\n\r\nThis should work, but I just add a CSS animation to zoom a bit when hovering a thumbnail:\r\n\r\n<style>\r\n  #lightgallery > a > img:hover {\r\n    transform: scale(1.2, 1.2);\r\n    transition: 0.2s ease-in-out;\r\n    cursor: pointer;\r\n  }\r\n<\/style>\r\n\r\nThat’s it for the proof of concept. Now let’s adapt it in R.\r\n\r\nClick to see the full HTML.\r\n\r\n<!doctype html>\r\n<html>\r\n  <head>\r\n    <link type=\"text/css\" rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/lightgallery-js/1.4.1-beta.0/css/lightgallery.css\" /> \r\n    <script src=\"https://cdnjs.cloudflare.com/ajax/libs/lightgallery-js/1.4.1-beta.0/js/lightgallery.min.js\"><\/script>\r\n\r\n   <!-- lightgallery plugins -->\r\n   <script src=\"https://cdnjs.cloudflare.com/ajax/libs/lg-fullscreen/1.2.1/lg-fullscreen.min.js\"><\/script>\r\n   <script src=\"https://cdnjs.cloudflare.com/ajax/libs/lg-thumbnail/1.2.1/lg-thumbnail.min.js\"><\/script>\r\n  <\/head>\r\n  <body>\r\n    <div id=\"lightgallery\">\r\n      <a href=\"img1.png\" data-sub-html=\"<h4>Sunset Serenity<\/h4><p>A gorgeous Sunset tonight captured at Coniston Water....<\/p>\">\r\n          <img src=\"thumb-img1.png\" />\r\n      <\/a>\r\n      <a href=\"img2.png\">\r\n          <img src=\"thumb-img2.png\" />\r\n      <\/a>\r\n    <\/div>\r\n\r\n    <script type=\"text/javascript\">\r\n      lightGallery(document.getElementById('lightgallery')); \r\n    <\/script>\r\n        \r\n    <style>\r\n      #lightgallery > a > img:hover {\r\n        transform: scale(1.2, 1.2);\r\n        transition: 0.2s ease-in-out;\r\n        cursor: pointer;\r\n      }\r\n    <\/style>\r\n  \r\n  <\/body>\r\n<\/html>\r\n\r\nMake the gallery with R\r\nCreate thumbnails\r\nFirst, store your (full-size) images in a folder, let’s say _gallery/img. As we saw above, lightgallery.js also requires thumbnails in addition to full-size images. To automatically create these thumbnails, we can use the function image_resize() in the package magick. First, I create a function to resize a single image, and I will apply it to all the images I have:\r\n\r\n\r\nlibrary(magick)\r\nlibrary(here)\r\n\r\nresize_image <- function(image) {\r\n  \r\n  imFile <- image_read(here::here(paste0(\"_gallery/img/\", image)))\r\n  imFile_resized <- magick::image_resize(imFile, \"6%\")\r\n  magick::image_write(imFile_resized, here::here(paste0(\"_gallery/img/thumb-\", image)))\r\n  \r\n}\r\n\r\nlist_png <- list.files(\"_gallery/img\")\r\nlapply(list_png, resize_image)\r\n\r\n\r\n\r\nBuild the HTML structure\r\nWe can now start building the HTML structure with the package htmltools. First, we can see that the HTML code for each image is very similar:\r\n\r\n<a href=\"img.png\">\r\n    <img src=\"thumb-img.png\" />\r\n<\/a>\r\n\r\nThis can be reproduced in R with:\r\n\r\n\r\nlibrary(htmltools)\r\n\r\ntags$a(\r\n  href = \"img.png\",\r\n  tags$img(src = \"thumb-img.png\")\r\n)\r\n\r\n\r\n\r\nWe can now create a function to apply this structure to all the images we have:\r\n\r\n\r\nmake_gallery_layout <- function() {\r\n  \r\n  # Get the names of all images\r\n  images <- list.files(\"_gallery/img\")\r\n  \r\n  # Get the names of all full-size images\r\n  images_full_size <- grep(\"thumb\", images, value = TRUE, invert = TRUE)\r\n  \r\n  # Get the names of all thumbnails\r\n  images_thumb <- grep(\"thumb\", images, value = TRUE)\r\n  \r\n  # Create a dataframe where each row is one image (useful for\r\n  # the apply() function)\r\n  images <- data.frame(images_thumb = images_thumb,\r\n                       images_full_size = images_full_size)\r\n  \r\n  # Create the HTML structure for each image\r\n  tagList(apply(images, 1, function(x) {\r\n      tags$a(\r\n        href = paste0(\"_gallery/img/\", x[[\"images_full_size\"]]),\r\n        tags$img(src = paste0(\"_gallery/img/\", x[[\"images_thumb\"]]))\r\n      )\r\n  }))\r\n  \r\n}\r\n\r\n\r\n\r\nLastly, we need to embed this HTML code in <div id=\"lightgallery\">, as shown in the first section. We can do that with the following code:\r\n\r\n\r\nwithTags(\r\n  div(\r\n    class = \"row\",\r\n    id = \"lightgallery\",\r\n    tagList(\r\n      make_gallery_layout()\r\n    )\r\n  )\r\n)\r\n\r\n\r\n\r\nWe now have all the HTML code we need. We now have to add the CSS and the JavaScript code. We can just copy-paste it in an R Markdown file.\r\n\r\nClick to see the full R Markdown file.\r\n---\r\ntitle: \"Gallery\"\r\noutput:\r\n  distill::distill_article\r\n---\r\n\r\n```{r echo = FALSE}\r\nknitr::opts_chunk$set(\r\n  echo = FALSE\r\n)\r\n```\r\n\r\n<head>\r\n\r\n<link type=\"text/css\" rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.0/css/lightgallery.min.css\" /> \r\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/lightgallery-js/1.4.1-beta.0/js/lightgallery.min.js\"><\/script>\r\n\r\n<!-- lightgallery plugins -->\r\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/lg-fullscreen/1.2.1/lg-fullscreen.min.js\"><\/script>\r\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/lg-thumbnail/1.2.1/lg-thumbnail.min.js\"><\/script>\r\n\r\n<\/head>\r\n\r\n```{css}\r\n#lightgallery > a > img:hover {\r\n   transform: scale(1.15, 1.15);\r\n   transition: 0.4s ease-in-out;\r\n   cursor: pointer;\r\n}\r\n```\r\n\r\n```{r include = FALSE}\r\n# Load the functions we have created\r\nsource(here::here(\"R/functions.R\"))\r\n```\r\n\r\n```{r}\r\n# Create layout\r\nwithTags(\r\n  div(\r\n    class = \"row\",\r\n    id = \"lightgallery\",\r\n    tagList(\r\n      make_gallery_layout()\r\n    )\r\n  )\r\n)\r\n\r\n```\r\n\r\n<script type=\"text/javascript\">\r\n    lightGallery(document.getElementById('lightgallery')); \r\n<\/script>\r\n\r\nUpdate GitHub Actions\r\nWe need to add fs::dir_copy(\"_gallery/img\", \"_site/_gallery/img\") in GitHub Actions so that the images are found when the gallery is built. We also have to add magick and httr in the list of packages to install.\r\nIf you haven’t set up GitHub Actions yet, you can check my previous post, or check my current GitHub Actions for this site.\r\nBonus: make a gallery for #tidytuesday\r\nI have started participating to #tidytuesday this year, and the main reason I wanted to create a gallery was to display my favorite plots. Therefore, I created a function to make it as easy as possible for me to update the plots I want to display in the gallery.\r\nThe purpose of the function below is to download a plot for a specific week in a specific year in the repo containing my plots.\r\n\r\n\r\nlibrary(httr)\r\n\r\nget_tt_image <- function(year, week) {\r\n  \r\n  if (is.numeric(year)) year <- as.character(year)\r\n  if (is.numeric(week)) week <- as.character(week)\r\n  if (nchar(week) == 1) week <- paste0(\"0\", week)\r\n  \r\n  ### Get the link to download the image I want\r\n  req <- GET(\"https://api.github.com/repos/etiennebacher/tidytuesday/git/trees/master?recursive=1\")\r\n  stop_for_status(req)\r\n  file_list <- unlist(lapply(content(req)$tree, \"[\", \"path\"), use.names = F)\r\n  png_list <- grep(\".png\", file_list, value = TRUE, fixed = TRUE)\r\n  png_wanted <- grep(year, png_list, value = TRUE)\r\n  png_wanted <- grep(paste0(\"W\", week), png_wanted, value = TRUE)\r\n  # If a png file is called accidental_art, don't take it\r\n  if (any(grepl(\"accidental_art\", png_wanted))) {\r\n    png_wanted <- png_wanted[-which(grepl(\"accidental_art\", png_wanted))]\r\n  }\r\n  \r\n  ### Link of the image I want to download\r\n  origin <- paste0(\r\n    \"https://raw.githubusercontent.com/etiennebacher/tidytuesday/master/\",\r\n    png_wanted \r\n  )\r\n  \r\n  ### Destination of this image\r\n  destination <- paste0(\"_gallery/img/\", year, \"-\", week, \"-\", trimws(basename(origin)))\r\n  \r\n  ### Download only if not already there\r\n  if (!file.exists(destination)) {\r\n    if (!file.exists(\"_gallery/img\")) {\r\n      dir.create(\"_gallery/img\")\r\n    }\r\n    download.file(origin, destination)\r\n  }\r\n  \r\n  ### Create the thumbnail if not already there\r\n  thumb_destination <- paste0(\"_gallery/img/thumb-\", year, \"-\", week, \"-\", \r\n                        trimws(basename(origin)))\r\n  if (!file.exists(thumb_destination)) {\r\n    resize_image(paste0(year, \"-\", week, \"-\", trimws(basename(origin))))\r\n  }\r\n \r\n}\r\n\r\n\r\n\r\nAs you can see, this function downloads the plot I want, puts it in _gallery/img and creates the thumbnail. All I have to do now is to choose the plots I want to display and to apply the function to these year-week pairs in the R Markdown file.\r\nNote that for some reason, this function sometimes fails on GitHub Actions because of HTTP error 403. I think this is related to the number of requests to GitHub API but what is strange is that this function isn’t supposed to make a lot of requests, so it is still a mystery.\r\n\r\nClick to see the full R Markdown file.\r\n---\r\ntitle: \"Gallery\"\r\noutput:\r\n  distill::distill_article\r\n---\r\n\r\n```{r echo = FALSE}\r\nknitr::opts_chunk$set(\r\n  echo = FALSE\r\n)\r\n```\r\n\r\n<head>\r\n\r\n<link type=\"text/css\" rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.0/css/lightgallery.min.css\" /> \r\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/lightgallery-js/1.4.1-beta.0/js/lightgallery.min.js\"><\/script>\r\n\r\n<!-- lightgallery plugins -->\r\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/lg-fullscreen/1.2.1/lg-fullscreen.min.js\"><\/script>\r\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/lg-thumbnail/1.2.1/lg-thumbnail.min.js\"><\/script>\r\n\r\n<\/head>\r\n\r\n```{css}\r\n#lightgallery > a > img:hover {\r\n   transform: scale(1.15, 1.15);\r\n   transition: 0.4s ease-in-out;\r\n   cursor: pointer;\r\n}\r\n```\r\n\r\n```{r include = FALSE}\r\n# Load the functions we have created\r\nsource(here::here(\"R/functions.R\"))\r\n\r\n# Make list of tidytuesday plots I want to show in the gallery\r\ntt_plots <- rbind(\r\n  c(2021, 8),\r\n  c(2021, 12),\r\n  c(2021, 13),\r\n  c(2021, 15),\r\n  c(2021, 16)\r\n)\r\n\r\n# Download the plots and create the thumbnails\r\napply(tt_plots, 1, function(x) get_tt_image(x[1], x[2]))\r\n```\r\n\r\n```{r}\r\n# Create layout\r\nwithTags(\r\n  div(\r\n    class = \"row\",\r\n    id = \"lightgallery\",\r\n    tagList(\r\n      make_gallery_layout()\r\n    )\r\n  )\r\n)\r\n\r\n```\r\n\r\n<script type=\"text/javascript\">\r\n    lightGallery(document.getElementById('lightgallery')); \r\n<\/script>\r\n\r\nConclusion\r\nIn this post, I tried to explain how to build a gallery with a simple example. However, you can also check the repo of my website to have a clearer view of how to do so. I also added some CSS styling that is not described here, to limit the code to what is really necessary.\r\nCheck the gallery to see the result.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-11-how-to-create-a-gallery-in-distill/gallery_image.jpg",
    "last_modified": "2022-05-03T17:54:55+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-19-use-github-actions-with-r-markdown-and-distill/",
    "title": "Use GitHub actions with R Markdown and Distill",
    "description": "How can you automatically render README, Distill website...",
    "author": [
      {
        "name": "Etienne Bacher",
        "url": {}
      }
    ],
    "date": "2021-03-19",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nRender a README\r\nRender a Distill website\r\n\r\n The preview image comes from: https://github.com/marketplace/actions/cancel-workflow-action \r\nSometimes, it is useful to automatically render an R Markdown document or a website, made with distill for example. In this post, I will present you two cases in which I use GitHub Actions to automatically do that.\r\nRender a README\r\nOne of my GitHub repos is a list of JavaScript libraries that have been adapted in R. You can find the repo here. I wanted this list to be easy to update, so that it can be done on GitHub directly. The idea is that when I (or someone else) find a JavaScript library that has been adapted into an R package, I add it to a CSV file on GitHub. The problem is that this CSV file is then used into an R Markdown file, that creates a clean README with all the information.\r\nWithout GitHub Actions, in addition to modify the CSV file, I would have to clone the repo, open it in RStudio, render the README, and push it back on GitHub.\r\nBut this task is repetitive: apart from the details I add to the CSV file, it can be automated. This is where GitHub Actions comes into play. The idea is that you create a .yml file that contains the R code you want to run to render the README. This is what mine looks like:\r\n\r\non:\r\n  push:\r\n    branches: master\r\n\r\nname: Render README\r\n\r\njobs:\r\n  render:\r\n    name: Render README\r\n    runs-on: macOS-latest\r\n    steps:\r\n      - uses: actions/checkout@v2\r\n      - uses: r-lib/actions/setup-r@v1\r\n      - uses: r-lib/actions/setup-pandoc@v1\r\n      - name: Install rmarkdown\r\n        run: Rscript -e 'install.packages(\"rmarkdown\")'\r\n      - name: Render README\r\n        run: Rscript -e 'rmarkdown::render(\"README.Rmd\", output_format = \"md_document\")'\r\n      - name: Commit results\r\n        run: |\r\n          git commit README.md -m 'Re-build README.Rmd' || echo \"No changes to commit\"\r\n          git push origin || echo \"No changes to commit\"\r\n\r\nThe first two parts are quite self-explanatory:\r\nthe jobs run every time there’s a push on the master branch;\r\nthe name of this process is “Render README”.\r\nThe third part needs a bit more details. There are some parts that I just copied and pasted from the R actions repository, but basically you can see that first it initiates R and pandoc (setup-r@v1, setup-pandoc@v1). Then, I run an R script to install the rmarkdown package and I use it to render the Rmd file to create README.md.\r\nLast but not least, GitHub Actions rendered the README, but the changes are not on the repo yet. Hence, the last step is to commit the changes with a message and to push them on the master branch. Now, every time I change the CSV file on the master branch, the README will be automatically rendered (after a few minutes, since all the actions have to run first).\r\nI said this was the .yaml file I use on my repo, but I lied a bit. Actually, for my list of JavaScript libraries to be up-to-date, I also need to scrape the htmlwidgets gallery once in a while. Hence, I use cron to run GitHub Actions every Monday at 00:00. See the documentation to know how to format your schedule. Finally, here’s the .yaml file I use:\r\n\r\non:\r\n  push:\r\n    branches: master\r\n  schedule:\r\n    - cron: '0 0 * * MON'\r\n\r\nname: Render README\r\n\r\njobs:\r\n  render:\r\n    name: Render README\r\n    runs-on: macOS-latest\r\n    steps:\r\n      - uses: actions/checkout@v2\r\n      - uses: r-lib/actions/setup-r@v1\r\n      - uses: r-lib/actions/setup-pandoc@v1\r\n      - name: Install rmarkdown\r\n        run: Rscript -e 'install.packages(\"rmarkdown\")'\r\n      - name: Render README\r\n        run: Rscript -e 'rmarkdown::render(\"README.Rmd\", output_format = \"md_document\")'\r\n      - name: Commit results\r\n        run: |\r\n          git commit README.md -m 'Re-build README.Rmd' || echo \"No changes to commit\"\r\n          git push origin || echo \"No changes to commit\"\r\n\r\nRender a Distill website\r\nTo automatically render your distill website on every push on master branch, the logic is very similar. In my .yml file, there are two main differences.\r\nThe first one is that I need to install more packages to render my distill website. Some will be essential for everyone (e.g distill) but other packages won’t be (e.g postcards).\r\nThe second one is more tricky, but could be useful for several people. Before using distill, I used blogdown. For some reasons (and mostly because distill is much easier in my opinion), I switched to distill. However, this switch changed a few URLs addresses, for my posts for instance. Therefore, I needed a _redirects file to, well, redirect the old URLs to the new ones and prevent 404 errors. The _redirects file needs to be in the _site folder, because it is the folder that is used by Netlify to build the site. The problem here is that this folder is deleted and re-generated every time rmarkdown::render_site() is called, i.e every time the website is locally built. Therefore, the _redirects file couldn’t just stay there. I had to add it manually after every build.\r\nThe solution to that is to automate this in GitHub Actions. After having rendered the website, I just copy _redirects from its location on the repo to the _site folder. Now, every time I change something on the master branch, the distill website is rebuilt, and then the _redirects file is added.\r\nOne drawback though: since these files are changed on GitHub only, the first thing you have to do when opening your site project in RStudio is to pull the changes (or, like me, you will struggle with merge conflicts).\r\nTo finish this post, here’s the .yml file for my distill website:\r\n\r\non:\r\n  push:\r\n    branches: master\r\n\r\nname: Render & Deploy Site\r\n\r\njobs:\r\n  build:\r\n    runs-on: macOS-latest\r\n    steps:\r\n      - uses: actions/checkout@v2\r\n\r\n      - uses: r-lib/actions/setup-r@master\r\n\r\n      - uses: r-lib/actions/setup-pandoc@master\r\n\r\n      - name: Install dependencies\r\n        run: |\r\n          install.packages(\"rmarkdown\")\r\n          install.packages(\"distill\")\r\n          install.packages(\"postcards\")\r\n          install.packages(\"devtools\")\r\n          install.packages(\"fs\")\r\n          devtools::install_github(\"etiennebacher/ebmisc\")\r\n        shell: Rscript {0}\r\n\r\n      - name: Render Site\r\n        run: Rscript -e 'rmarkdown::render_site(encoding = \"UTF-8\")'\r\n      - name: Copy redirects\r\n        run: Rscript -e 'fs::file_copy(\"_redirects\", \"_site/_redirects\")'\r\n      - name: Commit results\r\n        run: |\r\n          git add -A\r\n          git commit -m 'Rebuild site' || echo \"No changes to commit\"\r\n          git push origin || echo \"No changes to commit\"\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-03-19-use-github-actions-with-r-markdown-and-distill/gha_image.jpg",
    "last_modified": "2022-05-03T17:54:55+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-03-code-two-columns-blogdown/",
    "title": "Compare two languages in R Markdown documents",
    "description": "A few months ago, I tried to compare two languages in an article. Here are several ways to do it.",
    "author": [
      {
        "name": "Etienne Bacher",
        "url": {}
      }
    ],
    "date": "2020-12-03",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nTwo code chunks side-by-side\r\nCreate tabs with {xaringanExtra}\r\nUse <details>\r\n\r\nIn one of my first posts, I wanted to compare two languages (namely and R and Stata) by putting two chunks side-by-side. I asked how to do this on StackOverflow and continued to dig this question occasionally. I have now a few more or less convincing solutions/alternatives, that I summarize here.\r\nTwo code chunks side-by-side\r\nIt is possible to produce two columns in an HTML document (this is also possible for LaTeX files but the code is different). The following code…\r\n:::: {style=\"display: grid; grid-template-columns: 70% 70%; grid-column-gap: 30px;\"}\r\n\r\n::: {}\r\n```{r}\r\nhead(mtcars)\r\n```\r\n:::\r\n\r\n::: {}\r\n```{r}\r\nhead(anscombe)\r\n```\r\n:::\r\n\r\n::::\r\n… produces this output:\r\n\r\n\r\n\r\n\r\nhead(mtcars)\r\n\r\n\r\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\r\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\r\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\r\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\r\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\r\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\r\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\r\n\r\n\r\n\r\n\r\n\r\nhead(anscombe)\r\n\r\n\r\n  x1 x2 x3 x4   y1   y2    y3   y4\r\n1 10 10 10  8 8.04 9.14  7.46 6.58\r\n2  8  8  8  8 6.95 8.14  6.77 5.76\r\n3 13 13 13  8 7.58 8.74 12.74 7.71\r\n4  9  9  9  8 8.81 8.77  7.11 8.84\r\n5 11 11 11  8 8.33 9.26  7.81 8.47\r\n6 14 14 14  8 9.96 8.10  8.84 7.04\r\n\r\n\r\n\r\nAll of this is more detailed in this section of the R Markdown Cookbook.\r\nCreate tabs with {xaringanExtra}\r\nAn alternative to side-by-side chunks is to create tabs. We lose the ability to compare directly two chunks, but we can put much more tabs than code chunks. To do so, we use {xaringanExtra}, made by Garrick Aden-Buie. It is a great package that adds a lot of functionalities to R Markdown or {xaringan}.\r\nTo create tabs, we run xaringanExtra::use_panelset() first, and then we create the sections. Let’s init the panelset:\r\n\r\n\r\nlibrary(xaringanExtra)\r\n# enable panelset\r\nuse_panelset()\r\n\r\n\r\n\r\nNext, we can create several panels with ::::: {.panelset} and ::: {.panel}. Here’s an example:\r\n::::: {.panelset}\r\n\r\n::: {.panel}\r\n[mtcars]{.panel-name}\r\n```{r}\r\nhead(mtcars)\r\n```\r\n:::\r\n\r\n::: {.panel}\r\n[anscombe]{.panel-name}\r\n```{r}\r\nhead(anscombe)\r\n```\r\n:::\r\n\r\n::::\r\n\r\n\r\nmtcars\r\n\r\n\r\nhead(mtcars)\r\n\r\n\r\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\r\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\r\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\r\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\r\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\r\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\r\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\r\n\r\n\r\n\r\nanscombe\r\n\r\n\r\nhead(anscombe)\r\n\r\n\r\n  x1 x2 x3 x4   y1   y2    y3   y4\r\n1 10 10 10  8 8.04 9.14  7.46 6.58\r\n2  8  8  8  8 6.95 8.14  6.77 5.76\r\n3 13 13 13  8 7.58 8.74 12.74 7.71\r\n4  9  9  9  8 8.81 8.77  7.11 8.84\r\n5 11 11 11  8 8.33 9.26  7.81 8.47\r\n6 14 14 14  8 9.96 8.10  8.84 7.04\r\n\r\n\r\n\r\nUse <details>\r\nFinally, it is also possible to create chunks that are hidden by default but can be expanded by the user. This is particularly useful if you want to provide the user a reference. For instance, if you’re trying to teach a new language, it might be helpful to provide code that the user already knows, without displaying it by default.\r\nHere’s an example. We create the “expanding zone” below with:\r\n<details>\r\n<summary> Stata <\/summary>\r\nHere, I put some Stata code hidden so that the user can compare if necessary:\r\n```stata \r\nregress y x\r\n```\r\n<\/details>\r\n\r\n\r\n# Here I'm teaching R\r\nlm(mpg ~ drat, data = mtcars)\r\n\r\n\r\n\r\n\r\nStata\r\nHere, I put some Stata code hidden so that the user can compare if necessary:\r\nregress y x\r\n\r\nThat’s all! To summarize:\r\ncode chunks side-by-side\r\ntabs\r\nhidden chunks with <details>\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-03T17:54:55+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-10-18-nobel-laureates/",
    "title": "Visualize data on Nobel laureates per country",
    "description": "A post where I make animated graphs and maps to visualize the repartition of Nobel laureates per country.",
    "author": [
      {
        "name": "Etienne Bacher",
        "url": {}
      }
    ],
    "date": "2020-10-18",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nImport and clean data\r\nPlot the data\r\nStatic plot\r\nAnimated plots\r\n\r\nMaps\r\nStatic maps\r\nInteractive maps\r\n\r\n\r\nThe Nobel laureates of 2020 were announced last week, and I thought it would be interesting to visualize the repartition of laureates per country, as there are several ways to do so. I’m going to use this dataset available on Kaggle, which contains information on the year, category, name of the laureate, country, city and date of birth and death, among other things. Notice that this dataset goes from 1901 to 2016 and therefore doesn’t contain the most recent laureates.\r\nBut first of all, we need to load all the packages we will use in this analysis:\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(janitor)\r\nlibrary(ggthemes)\r\nlibrary(gganimate)\r\nlibrary(here)\r\nlibrary(tmap)\r\nlibrary(countrycode)\r\nlibrary(echarts4r)\r\n\r\n\r\n\r\nImport and clean data\r\nNow, we can import the dataset. To remove the capital letters and transform the column names in snake case (i.e names such as “column_name” instead of “Column Name”), we can use the function clean_names() of the package {janitor} 1:\r\n\r\n\r\nnobel_laureates_raw <- read_csv(here(\"_posts/2020-10-18-nobel-laureates/nobel-laureates.csv\")) %>%\r\n  janitor::clean_names()\r\n\r\n\r\n\r\nThe first thing that we have to correct before doing visualization concerns the country names. Indeed, many countries have changed since 1901. For example, Czechoslovakia no longer exists, as well as Prussia. In this dataset, the columns containing country names display first the official name at the time, and then put the current name of the country between brackets.\r\n\r\n# A tibble: 6 x 2\r\n  birth_country     death_country\r\n  <chr>             <chr>        \r\n1 Netherlands       Germany      \r\n2 France            France       \r\n3 Prussia (Poland)  Germany      \r\n4 Switzerland       Switzerland  \r\n5 France            France       \r\n6 Prussia (Germany) Germany      \r\n\r\nSince we only want the current country names, we must modify these columns so that:\r\nif the name doesn’t have brackets (i.e the country hasn’t changed in time), we let it as-is;\r\nif the name has brackets (i.e the country has changed), we only want to keep the name between brackets.\r\nSince I must do this for two columns (birth_country and death_country), I created a function (and this was the perfect example of losing way too much time by making a function to save time…):\r\n\r\n\r\nclean_country_names <- function(data, variable) {\r\n  data <- data %>%\r\n    mutate(\r\n      x = gsub(\r\n        \"(?<=\\\\()[^()]*(?=\\\\))(*SKIP)(*F)|.\",\r\n        \"\",\r\n        {{variable}},\r\n        perl = T\r\n      ),\r\n      x = ifelse(x == \"\", {{variable}}, x)\r\n    ) %>%\r\n    select(- {{variable}}) %>%\r\n    rename({{variable}} := \"x\")\r\n}\r\n\r\n\r\n\r\nThis function takes a dataset (data), and creates a new column (x) that will take the name between brackets if original variable has brackets, or the unique name if the original variable doesn’t have brackets. Then, x is renamed as the variable we specified first. I must admit that regular expressions (such as the one in gsub()) continue to be a big mystery for me, and I thank StackOverflow for providing many examples.\r\nNow, we apply this function to our columns with countries:\r\n\r\n\r\nnobel_laureates <- clean_country_names(nobel_laureates_raw, birth_country)\r\nnobel_laureates <- clean_country_names(nobel_laureates, death_country)\r\n\r\n\r\n\r\nThe country names are now cleaned:\r\n\r\n# A tibble: 6 x 2\r\n  birth_country death_country\r\n  <chr>         <chr>        \r\n1 Netherlands   Germany      \r\n2 France        France       \r\n3 Poland        Germany      \r\n4 Switzerland   Switzerland  \r\n5 France        France       \r\n6 Germany       Germany      \r\n\r\nFrom now on, there are several ways to visualize the repartition of Nobel laureates per country. We could do a static bar plot, an animated bar plot to see the evolution in time, a static map, or an interactive map.\r\nPlot the data\r\nStatic plot\r\nFirst of all, we need to compute the number of Nobel laureates per country:\r\n\r\n\r\nnobel_per_country <- nobel_laureates %>%\r\n  select(birth_country, full_name) %>%\r\n  distinct() %>%\r\n  group_by(birth_country) %>%\r\n  count(sort = TRUE) %>%\r\n  ungroup() %>%\r\n  drop_na()\r\n\r\n\r\n\r\nThen we can plot this number, only for the first 20 countries (so that the plot can be readable):\r\n\r\n\r\nnobel_per_country %>%\r\n  select(birth_country, n) %>%\r\n  top_n(20) %>%\r\n  mutate(birth_country = reorder(birth_country, n)) %>%\r\n  ggplot(aes(x = birth_country, y = n)) +\r\n  geom_col() +\r\n  coord_flip() +\r\n  xlab(\"Country\") +\r\n  ylab(\"\") +\r\n  geom_text(aes(label = n), nudge_y = 10) +\r\n  ggthemes::theme_clean()\r\n\r\n\r\n\r\n\r\nWe can also check the repartition per country and per category:\r\n\r\n\r\n# The 20 countries with the most nobels\r\ntop_20 <- nobel_per_country %>%\r\n  top_n(10) %>%\r\n  select(birth_country) %>%\r\n  unlist(use.names = FALSE)\r\n\r\nnobel_laureates %>%\r\n  select(birth_country, full_name, category) %>%\r\n  distinct() %>%\r\n  group_by(birth_country, category) %>%\r\n  mutate(n = n()) %>%\r\n  ungroup() %>%\r\n  drop_na() %>%\r\n  select(- full_name) %>%\r\n  distinct() %>%\r\n  filter(birth_country %in% top_20) %>%\r\n  ggplot(aes(x = birth_country, y = n)) +\r\n  geom_col() +\r\n  coord_flip() +\r\n  xlab(\"Country\") +\r\n  ylab(\"\") +\r\n  geom_text(aes(label = n), nudge_y = 10) +\r\n  ggthemes::theme_clean() +\r\n  facet_wrap(~category)\r\n\r\n\r\n\r\n\r\nAnimated plots\r\nTo observe the evolution of this number in time, one way would be to plot lines with year in x-axis. But we could also keep the first plot we made and animate it with {gganimate}.\r\nFirst, we compute the cumulated sum of Nobel laureates. Indeed, the number of laureates per year is useless for us, we want to see the evolution of the total number:\r\n\r\n\r\nnobel_per_country_year <- nobel_laureates %>%\r\n  select(year, birth_country) %>%\r\n  group_by(year, birth_country) %>%\r\n  count(sort = TRUE) %>%\r\n  ungroup() %>%\r\n  drop_na() %>%\r\n  arrange(birth_country, year) %>%\r\n  complete(year, birth_country) %>%\r\n  mutate(n = ifelse(is.na(n), 0, n),\r\n         year = as.integer(year)) %>%\r\n  filter(birth_country %in% top_20) %>%\r\n  group_by(birth_country) %>%\r\n  mutate(n_cumul = cumsum(n)) %>%\r\n  arrange(birth_country)\r\n\r\n\r\n\r\nThen, we use almost the same code as for the first plot, but we add arguments at the end that tell how we want the animation to be:\r\n\r\n\r\nplot_evol <- nobel_per_country_year %>%\r\n  select(birth_country, year, n_cumul) %>%\r\n  filter((year %% 2) != 0) %>%\r\n  ggplot(aes(x = reorder(birth_country, n_cumul), y = n_cumul)) +\r\n  geom_col() +\r\n  coord_flip() +\r\n  xlab(\"Country\") +\r\n  ylab(\"\") +\r\n  geom_text(aes(label = as.character(round(n_cumul, 0))), nudge_y = 10) +\r\n  ggthemes::theme_clean() +\r\n  transition_time(year) +\r\n  ggtitle(\"Year: {frame_time}\") +\r\n  ease_aes('linear', interval = 2)\r\n\r\nanimate(plot_evol, duration = 15, fps = 20)\r\n\r\n\r\n\r\n\r\nThis allows us to see that the USA have seen their number of Nobel laureates surge from the 1960’s and 1970’s, which corresponds more or less to the creation of the so-called “Nobel Prize in Economics” in 1969. The plot per category also indicates that this prize plays a major role in the domination of the USA.\r\nMaps\r\nStatic maps\r\nTo create maps, we rely on the package {tmap}. In addition to its functions, this package also gives access to a dataset that we will use to show the number of laureates per country.\r\n\r\n\r\ndata(World)\r\n\r\n\r\n\r\nWe need to merge our dataset of Nobel laureates with this dataset. But the country names differ. Therefore, we have to use ISO codes instead. World already contains ISO codes, so we only have to create those for our dataset. This can be done very easily with the package {countrycode}. However, some countries in our dataset don’t have ISO codes, such as Scotland, Northern Ireland or Czechoslovakia. The two former can be recoded as United Kingdom, but Czechoslovakia was located on current Slovakia, Czech Republic and Ukraine, so we drop it of our dataset.\r\n\r\n\r\nnobel_per_country <- nobel_per_country %>%\r\n  mutate(\r\n    iso_birth = countrycode(birth_country, origin = \"country.name\", destination = \"iso3c\"),\r\n    iso_birth = case_when(\r\n      birth_country == \"Scotland\" | birth_country == \"Northern Ireland\" ~ \"GBR\",\r\n      TRUE ~ iso_birth\r\n    )\r\n  )\r\n\r\n\r\n\r\nWe can now merge the two datasets based on their ISO codes…\r\n\r\n\r\nWorld <- World %>%\r\n  full_join(nobel_per_country, by = c(\"iso_a3\" = \"iso_birth\")) %>%\r\n  rename(\"number\" = \"n\") %>%\r\n  mutate(number = ifelse(is.na(number), 0, number))\r\n\r\n\r\n\r\n… and we can build the map and fill the countries with the number of laureates:\r\n\r\n\r\ntm_shape(World, projection = 4326) +\r\n  tm_fill(\"number\", breaks = c(0, 5, 10, 50, 200, Inf), palette = \"YlOrBr\") +\r\n  tm_polygons() +\r\n  tm_legend(title = \"Nobel prizes per country\", legend.title.size = 10^(-4)) +\r\n  tm_layout(legend.outside = TRUE)\r\n\r\n\r\n\r\n\r\nInteractive maps\r\nFinally, we will make interactive maps with {echarts4r}. Firstly, let’s make an identical map as the one above but with a few interactive features.\r\n{echarts4r} uses specific country names, so we use once again {countrycode} to modify the names in our dataset.\r\n\r\n\r\nnobel_per_country_echarts <- e_country_names(data = nobel_per_country,\r\n                                             input = iso_birth,\r\n                                             type = \"iso3c\")\r\n\r\n\r\n\r\nNow we can plot the map:\r\n\r\n\r\nnobel_per_country_echarts %>%\r\n  e_charts(iso_birth) %>%\r\n  e_map(n, roam = TRUE) %>%\r\n  e_visual_map(max = max(nobel_per_country_echarts$n))\r\n\r\n\r\n\r\n\r\nHovering the countries gives us their name, and the number of laureates in the legend. We can also zoom in and out. We could see the evolution of laureates in time with timeline = TRUE:\r\n\r\n\r\nnobel_per_country_year_map <- nobel_laureates %>%\r\n  select(year, birth_country) %>%\r\n  group_by(year, birth_country) %>%\r\n  count(sort = TRUE) %>%\r\n  ungroup() %>%\r\n  drop_na() %>%\r\n  arrange(birth_country, year) %>%\r\n  complete(year, birth_country) %>%\r\n  mutate(n = ifelse(is.na(n), 0, n),\r\n         year = as.integer(year)) %>%\r\n  group_by(birth_country) %>%\r\n  mutate(n_cumul = cumsum(n)) %>%\r\n  arrange(birth_country)\r\n\r\nnobel_per_country_year_map <- nobel_per_country_year_map %>%\r\n  mutate(\r\n    iso_birth = countrycode(birth_country, origin = \"country.name\", destination = \"iso3c\"),\r\n    iso_birth = case_when(\r\n      birth_country == \"Scotland\" | birth_country == \"Northern Ireland\" ~ \"GBR\",\r\n      TRUE ~ iso_birth\r\n    )\r\n  )\r\n\r\nnobel_per_country_year_echarts <- e_country_names(data = nobel_per_country_year_map,\r\n                                                  input = iso_birth,\r\n                                                  type = \"iso3c\")\r\n\r\nnobel_per_country_year_echarts %>%\r\n  group_by(year) %>%\r\n  e_charts(iso_birth, timeline = TRUE) %>%\r\n  e_map(n_cumul, roam = TRUE) %>%\r\n  e_visual_map(max = 257) %>%\r\n  e_timeline_opts(\r\n    playInterval = 250,\r\n    symbol = \"none\"\r\n  )\r\n\r\n\r\n\r\n\r\n\r\nAnd that’s it! I used data about Nobel laureates to present a few plots and maps made with {ggplot2}, {gganimate}, {tmap}, and {echarts4r}. I used these packages but there are countless ways to make plots or maps, whether static or interactive, with R:\r\nplots: base R, {highcharter}, {charter}, {plotly}, etc.\r\nmaps: base R, {leaflet}, {sf}, {ggmap}, etc.\r\nI hope you enjoyed it!\r\n\r\nSession Info\r\nThis is my session info, so that you can see the versions of packages used. This is useful if the results in my post are no longer reproducible because packages changed. The packages with a star (*) are those explicitely called in the script.\r\n\r\n─ Session info ─────────────────────────────────────────────────────\r\n setting  value                       \r\n version  R version 4.0.3 (2020-10-10)\r\n os       Ubuntu 18.04.5 LTS          \r\n system   x86_64, linux-gnu           \r\n ui       X11                         \r\n language en                          \r\n collate  fr_FR.UTF-8                 \r\n ctype    fr_FR.UTF-8                 \r\n tz       Europe/Paris                \r\n date     2021-01-20                  \r\n\r\n─ Packages ─────────────────────────────────────────────────────────\r\n package      * version  date       lib\r\n abind          1.4-5    2016-07-21 [1]\r\n assertthat     0.2.1    2019-03-21 [1]\r\n backports      1.2.1    2020-12-09 [1]\r\n base64enc      0.1-3    2015-07-28 [1]\r\n broom          0.7.3    2020-12-16 [1]\r\n cellranger     1.1.0    2016-07-27 [1]\r\n class          7.3-17   2020-04-26 [4]\r\n classInt       0.4-3    2020-04-07 [1]\r\n cli            2.2.0    2020-11-20 [1]\r\n codetools      0.2-16   2018-12-24 [4]\r\n colorspace     2.0-0    2020-11-11 [1]\r\n countrycode  * 1.2.0    2020-05-22 [1]\r\n crayon         1.3.4    2017-09-16 [1]\r\n crosstalk      1.1.1    2021-01-12 [1]\r\n DBI            1.1.1    2021-01-15 [1]\r\n dbplyr         2.0.0    2020-11-03 [1]\r\n dichromat      2.0-0    2013-01-24 [1]\r\n digest         0.6.27   2020-10-24 [1]\r\n distill        1.2      2021-01-13 [1]\r\n downlit        0.2.1    2020-11-04 [1]\r\n dplyr        * 1.0.3    2021-01-15 [1]\r\n e1071          1.7-4    2020-10-14 [1]\r\n echarts4r    * 0.3.4    2020-10-29 [1]\r\n ellipsis       0.3.1    2020-05-15 [1]\r\n evaluate       0.14     2019-05-28 [1]\r\n fansi          0.4.2    2021-01-15 [1]\r\n farver         2.0.3    2020-01-16 [1]\r\n fastmap        1.0.1    2019-10-08 [1]\r\n forcats      * 0.5.0    2020-03-01 [1]\r\n fs             1.5.0    2020-07-31 [1]\r\n generics       0.1.0    2020-10-31 [1]\r\n gganimate    * 1.0.7    2020-10-15 [1]\r\n ggplot2      * 3.3.3    2020-12-30 [1]\r\n ggthemes     * 4.2.0    2019-05-13 [1]\r\n gifski         0.8.6    2018-09-28 [1]\r\n glue           1.4.2    2020-08-27 [1]\r\n gtable         0.3.0    2019-03-25 [1]\r\n haven          2.3.1    2020-06-01 [1]\r\n here         * 1.0.1    2020-12-13 [1]\r\n hms            1.0.0    2021-01-13 [1]\r\n htmltools      0.5.1    2021-01-12 [1]\r\n htmlwidgets    1.5.3    2020-12-10 [1]\r\n httpuv         1.5.5    2021-01-13 [1]\r\n httr           1.4.2    2020-07-20 [1]\r\n janitor      * 2.1.0    2021-01-05 [1]\r\n jsonlite       1.7.2    2020-12-09 [1]\r\n KernSmooth     2.23-17  2020-04-26 [4]\r\n knitr          1.30     2020-09-22 [1]\r\n labeling       0.4.2    2020-10-20 [1]\r\n later          1.1.0.1  2020-06-05 [1]\r\n lattice        0.20-41  2020-04-02 [4]\r\n leafem         0.1.3    2020-07-26 [1]\r\n leaflet        2.0.4.1  2021-01-07 [1]\r\n leafsync       0.1.0    2019-03-05 [1]\r\n lifecycle      0.2.0    2020-03-06 [1]\r\n lubridate      1.7.9.2  2020-11-13 [1]\r\n lwgeom         0.2-5    2020-06-12 [1]\r\n magrittr       2.0.1    2020-11-17 [1]\r\n mime           0.9      2020-02-04 [1]\r\n modelr         0.1.8    2020-05-19 [1]\r\n munsell        0.5.0    2018-06-12 [1]\r\n pillar         1.4.7    2020-11-20 [1]\r\n pkgconfig      2.0.3    2019-09-22 [1]\r\n png            0.1-7    2013-12-03 [1]\r\n prettyunits    1.1.1    2020-01-24 [1]\r\n progress       1.2.2    2019-05-16 [1]\r\n promises       1.1.1    2020-06-09 [1]\r\n purrr        * 0.3.4    2020-04-17 [1]\r\n R6             2.5.0    2020-10-28 [1]\r\n raster         3.4-5    2020-11-14 [1]\r\n RColorBrewer   1.1-2    2014-12-07 [1]\r\n Rcpp           1.0.6    2021-01-15 [1]\r\n readr        * 1.4.0    2020-10-05 [1]\r\n readxl         1.3.1    2019-03-13 [1]\r\n reprex         0.3.0    2019-05-16 [1]\r\n rlang          0.4.10   2020-12-30 [1]\r\n rmarkdown      2.6.4    2021-01-19 [1]\r\n rprojroot      2.0.2    2020-11-15 [1]\r\n rstudioapi     0.13     2020-11-12 [1]\r\n rvest          0.3.6    2020-07-25 [1]\r\n scales         1.1.1    2020-05-11 [1]\r\n sessioninfo    1.1.1    2018-11-05 [1]\r\n sf           * 0.9-6    2020-09-13 [1]\r\n shiny          1.5.0    2020-06-23 [1]\r\n snakecase      0.11.0   2019-05-25 [1]\r\n sp             1.4-5    2021-01-10 [1]\r\n stars          0.4-3    2020-07-08 [1]\r\n stringi        1.5.3    2020-09-09 [1]\r\n stringr      * 1.4.0    2019-02-10 [1]\r\n tibble       * 3.0.5    2021-01-15 [1]\r\n tidyr        * 1.1.2    2020-08-27 [1]\r\n tidyselect     1.1.0    2020-05-11 [1]\r\n tidyverse    * 1.3.0    2019-11-21 [1]\r\n tmap         * 3.2      2020-09-15 [1]\r\n tmaptools      3.1      2020-07-01 [1]\r\n tweenr         1.0.1    2018-12-14 [1]\r\n units          0.6-7    2020-06-13 [1]\r\n utf8           1.1.4    2018-05-24 [1]\r\n vctrs          0.3.6    2020-12-17 [1]\r\n viridisLite    0.3.0    2018-02-01 [1]\r\n withr          2.4.0    2021-01-16 [1]\r\n xfun           0.20     2021-01-06 [1]\r\n XML            3.99-0.5 2020-07-23 [1]\r\n xml2           1.3.2    2020-04-23 [1]\r\n xtable         1.8-4    2019-04-21 [1]\r\n yaml           2.2.1    2020-02-01 [1]\r\n source                              \r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.3)                      \r\n Github (JohnCoene/echarts4r@082e62c)\r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.2)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.2)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.2)                      \r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.2)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.1)                      \r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.2)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.1)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.3)                      \r\n Github (rstudio/rmarkdown@2e8572e)  \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.2)                      \r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.2)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.2)                      \r\n CRAN (R 4.0.2)                      \r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.2)                      \r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.2)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.2)                      \r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.1)                      \r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.2)                      \r\n CRAN (R 4.0.3)                      \r\n CRAN (R 4.0.0)                      \r\n CRAN (R 4.0.0)                      \r\n\r\n[1] /home/etienne/R/x86_64-pc-linux-gnu-library/4.0\r\n[2] /usr/local/lib/R/site-library\r\n[3] /usr/lib/R/site-library\r\n[4] /usr/lib/R/library\r\n\r\n\r\nThis function is very useful even when column names are much more messy.↩︎\r\n",
    "preview": "posts/2020-10-18-nobel-laureates/distill-preview.png",
    "last_modified": "2022-05-03T17:54:55+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-10-04-the-americans/",
    "title": "Text analysis of the dialogues in \"The Americans\"",
    "description": "I love the TV show \"The Americans\" and decided to harvest and analyze the dialogues.",
    "author": [
      {
        "name": "Etienne Bacher",
        "url": {}
      }
    ],
    "date": "2020-10-04",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nGet the dialogues\r\nFind a source for the dialogues\r\nImport the dialogues\r\nGeneralize this to all episodes\r\n\r\nAnalyze the dialogues\r\nNumber of words per episode per season\r\nWhat words are the most said?\r\nSentiment analysis\r\n\r\n\r\nThe Americans is a TV show that tells the story of two KGB agents infiltrated in the USA in the 1980’s. It shows us the life of these two people, who have to do espionage missions while taking care of their children and developing their own business as their cover.\r\nRecently, I watched the talk of Ryan Timpe at RStudio::conf 2020 about learning R with humorous side projects. It made me think about what projects I could develop to learn new things with R, and it pushed me to mix my interest both for The Americans and for R. I thought it would be interesting to analyze the dialogues of this TV show since it required learning two new skills: scraping web data to get the dialogues, and doing text analysis to explore these dialogues.\r\nGet the dialogues\r\nFind a source for the dialogues\r\nApparently, contrarily to Friends, nobody developed a package containing the dialogues of The Americans yet. Therefore, I had to search online for these, and I found this website that contains all of the dialogues and other text information (lyrics, stage directions, etc.), with one page for one episode.\r\nThis website doesn’t provide the dialogues for the end of season 6. However, this is not a big issue. Another drawback of this website is that it doesn’t always provide information on who is talking, so it’s not possible to analyze the words of a specific person. But it’s good enough for me, I just want to train, the results don’t matter here.\r\nImport the dialogues\r\nLet’s see how to import the dialogues with episode 1 of season 1. First of all, two packages will be needed:\r\n\r\n\r\nlibrary(rvest)\r\nlibrary(tidyverse)\r\n\r\n\r\n\r\nNow, we want to obtain the details of the page containing the dialogues for the first episode:\r\n\r\n\r\npage <- xml2::read_html(\"http://transcripts.foreverdreaming.org/viewtopic.php?f=116&t=15871\")\r\n\r\n\r\n\r\nThis condenses all HTML information into two lists. But we only need the dialogues. Therefore, we have to find the HTML argument that contains them. To do so, we can use the Inspector in the webpage (Ctrl+Shift+C). When hovering elements on the webpage, we can see that there are several classes. Each line is embedded into p, but we notice that the whole text is in div.postbody.\r\n\r\nTherefore, we can select only this class:\r\n\r\n\r\npage_text <- html_node(page, \"div.postbody\") %>%\r\n  html_children() %>%\r\n  xml2::xml_find_all(\"//div[contains(@class, 'postbody')]\") %>%\r\n  html_text(trim = TRUE)\r\n\r\n\r\n\r\nNow, page_text is a very long vector that contains all text information. However, everything is not important here: we don’t want to analyze the lyrics of the soundtrack, the stage directions, or the name of the person who is talking. The latter could be interesting if we had it for every sentence. However, we only have it occasionally, which makes it useless. To remove this irrelevant text, we will use gsub(), a base R function.\r\n\r\n\r\npage_text_cleaned <- page_text %>%\r\n  gsub(\"♪♪\", \"\", .) %>% # double music symbol\r\n  gsub(\"♪ [^♪]+♪\", \"\", .) %>% # text between music symbol (= lyrics)\r\n  gsub(\"\\\\n\", \" \", .) %>% # new line symbol\r\n  gsub(\"\\\\t\", \" \", .) %>% # \\t\r\n  gsub(\"\\\\[[^\\\\]]*\\\\]\", \"\", ., perl = TRUE) %>% # text between brackets\r\n  gsub(\"\\\\([^\\\\)]*\\\\)\", \"\", ., perl = TRUE) %>% # text between parenthesis\r\n  gsub(\"\\\\\\\\\", \"\", .) %>% # backslash\r\n  gsub(\"\\\\(adsbygoogle = window.adsbygoogle \\\\|\\\\| \\\\).push\\\\(\\\\{\\\\}\\\\);\", \"\", .) %>% # ads\r\n  gsub(\"Philip:\", \"\", .) %>% \r\n  gsub(\"Elizabeth:\", \"\", .) %>%\r\n  gsub(\"Paige:\", \"\", .) %>%\r\n  gsub(\"Henry:\", \"\", .) %>%\r\n  gsub(\"Stan:\", \"\", .) \r\n\r\n\r\n\r\nThe text is now cleaned: most of the useless text was removed, and 99% of what’s left is the dialogues.\r\nGeneralize this to all episodes\r\nAs I said before, there is a different page for each episode. How can we generalize the previous step to all these pages?\r\nWell, the code we used before will be the same once we have the HTML information of the episode page. The only problem here is that we must find a way to download this HTML information for all pages. We can notice that the URL addresses are almost identical for all episodes:\r\nEpisode 1, season 1: http://transcripts.foreverdreaming.org/viewtopic.php?f=116&t=15871\r\nEpisode 2, season 1: http://transcripts.foreverdreaming.org/viewtopic.php?f=116&t=15872\r\nWe see here that only the argument t differs between those addresses. If we could collect all the values of t for all the pages, we could then collect the HTML information very easily. There is now another problem: what are the values of t? We could suppose that we just need to add 1 to the previous number (15871, 15872, 15873…). However, for episode 12 in season 4 for example, the value of t is 27447. Therefore, we must find another way to collect these t values.\r\nCollect t values\r\nTo do so, we use the Inspector once again, but this time on the home page, not on an episode page. Exploring the HTML tags, we notice that the t value is displayed in the class a.topictitle, among other information.\r\nSee the value of t in href in the inspector?Using the same code as before, we collect the text contained in this class, and extract the t value:\r\n\r\n\r\n# First we get the HTML information of the home page (menu 1)\r\nmenu_1 <- read_html(\"http://transcripts.foreverdreaming.org/viewforum.php?f=116\")\r\n\r\n# Then we obtain the text in this specific class and keep only the t value\r\nepisode_ids_1 <- html_node(menu_1, \"body\") %>%\r\n  html_children() %>%\r\n  xml2::xml_find_all(\"//a[contains(@class, 'topictitle')]\") %>%\r\n  sub('.*f=116&amp;t= *(.*?) *&amp;sid.*', \"\\\\1\", .) %>%\r\n  as.numeric()\r\n\r\n\r\n\r\nThe last step for this first menu is to remove the two first t values (or ids), since they correspond to “Online Store” and “Board Updates” and not to episodes.\r\n\r\n\r\nepisode_ids_1 <- episode_ids_1[-c(1, 2)]\r\n\r\n\r\n\r\nAfter repeating this operation for the other two menus, we finally obtain all the episode ids:\r\n\r\n\r\n### Second menu\r\nmenu_2 <- read_html(\"http://transcripts.foreverdreaming.org/viewforum.php?f=116&start=25\")\r\n\r\nepisode_ids_2 <- html_node(menu_2, \"body\") %>%\r\n  html_children() %>%\r\n  xml2::xml_find_all(\"//a[contains(@class, 'topictitle')]\") %>%\r\n  sub('.*f=116&amp;t= *(.*?) *&amp;sid.*', \"\\\\1\", .) %>%\r\n  as.numeric() \r\n\r\nepisode_ids_2 <- episode_ids_2[-c(1, 2)]\r\n\r\n### Third menu\r\n\r\nmenu_3 <- read_html(\"http://transcripts.foreverdreaming.org/viewforum.php?f=116&start=50\")\r\n\r\nepisode_ids_3 <- html_node(menu_3, \"body\") %>%\r\n  html_children() %>%\r\n  xml2::xml_find_all(\"//a[contains(@class, 'topictitle')]\") %>%\r\n  sub('.*f=116&amp;t= *(.*?) *&amp;sid.*', \"\\\\1\", .) %>%\r\n  as.numeric()\r\n\r\nepisode_ids_3 <- episode_ids_3[-c(1, 2)]\r\n\r\n\r\n### All episodes ids\r\nepisode_ids_total <- c(episode_ids_1, episode_ids_2, episode_ids_3)\r\n\r\n\r\n\r\nGet the dialogues for all episodes\r\nThe final step in this first part (obtaining the dialogues) is to get the dialogues for all pages. We already have the code for that, it is the first thing we made. Now, we only have to apply this code for each page, i.e for each episode id we got. To do so, we can use the {purrr} package (contained in the {tidyverse}):\r\n\r\n\r\nlist_dialogues <- purrr::map(episode_ids_total, .f = function(x) {\r\n  \r\n  page <- read_html(paste0(\"http://transcripts.foreverdreaming.org/viewtopic.php?f=116&t=\", x))\r\n  \r\n  page_text <- html_node(page, \"div.postbody\") %>%\r\n    html_children() %>%\r\n    xml2::xml_find_all(\"//div[contains(@class, 'postbody')]\") %>%\r\n    html_text(trim = TRUE)\r\n \r\n  page_text_cleaned <- page_text %>%\r\n    gsub(\"♪♪\", \"\", .) %>%\r\n    gsub(\"♪ [^♪]+♪\", \"\", .) %>%\r\n    gsub(\"\\\\n\", \" \", .) %>%\r\n    gsub(\"\\\\t\", \" \", .) %>%\r\n    gsub(\"\\\\[[^\\\\]]*\\\\]\", \"\", ., perl = TRUE) %>%\r\n    gsub(\"\\\\([^\\\\)]*\\\\)\", \"\", ., perl = TRUE) %>%\r\n    gsub(\"\\\\\\\\\", \"\", .) %>%\r\n    gsub(\"\\\\(adsbygoogle = window.adsbygoogle \\\\|\\\\| \\\\).push\\\\(\\\\{\\\\}\\\\);\", \"\", .) %>%\r\n    gsub(\"Philip:\", \"\", .) %>%\r\n    gsub(\"Elizabeth:\", \"\", .) %>%\r\n    gsub(\"Paige:\", \"\", .) %>%\r\n    gsub(\"Henry:\", \"\", .) %>%\r\n    gsub(\"Stan:\", \"\", .) \r\n  \r\n  return(page_text_cleaned)\r\n  \r\n})\r\n\r\n\r\n\r\nWe used the function map(), which takes two arguments: a list as input, and a function. The list we used is the list of all episode ids. The function is a custom function: it gathers the code we wrote previously and runs it for each episode id.\r\nWe now have list_dialogues, a list that contains 52 big character vectors (one per episode), corresponding to all the dialogues of the episodes.\r\nAnalyze the dialogues\r\nNow that we have all the dialogues for each episode, we can analyze them with the {tidytext} package. First, we must convert the big character vectors into tidy tibbles, i.e dataframes with one word per row. This can be done with unnest_tokens(), that we use in map() once again:\r\n\r\n\r\nlibrary(tidytext)\r\n\r\nlist_dialogues_words <- map(list_dialogues, .f = function(x) {\r\n  as_tibble(x) %>%\r\n    unnest_tokens(word, value)\r\n})  \r\n\r\n\r\n\r\nNumber of words per episode per season\r\nWith this data, we can firstly see the number of words in each episode. To do so, we compute the number of rows (i.e words) per episode and then we plot them as lines to see the evolution throughout a season.\r\n\r\n\r\nmap(list_dialogues_words, nrow) %>%\r\n  unlist() %>%\r\n  as_tibble() %>%\r\n  tibble::rowid_to_column() %>%\r\n  mutate(\r\n    season = case_when(\r\n      rowid %in% c(1:13) ~ 1,\r\n      rowid %in% c(14:26) ~ 2,\r\n      rowid %in% c(27:39) ~ 3,\r\n      rowid %in% c(40:52) ~ 4,\r\n      rowid %in% c(53:65) ~ 5,\r\n      rowid %in% c(65:70) ~ 6\r\n    )\r\n  ) %>%\r\n  mutate(rowid = c(rep(c(1:13), 5), 1:5)) %>%\r\n  ungroup() %>%\r\n  ggplot(aes(x = rowid, y = value, color = factor(season))) + \r\n  geom_line() +\r\n  scale_x_discrete(name = \"Episode number\", limits = factor(c(1:13))) +\r\n  scale_y_continuous(name = \"Number of words\") +\r\n  ggtitle(\"Number of words per episode per season\") +\r\n  scale_color_discrete(name = \"Season\")\r\n\r\n\r\n\r\n\r\nWe can see that the first episode of the first season is by far the one with the most words. Maybe because the first one introduces us to the situation, the plot? It is also hard to verify that we didn’t remove words by mistake in the dialogues of the other episodes.\r\nWhat words are the most said?\r\nThe {tidytext} package also allows us to identify the words that are the most used in the show. We will focus here on the first season but the code below can be easily adapted to analyze the other seasons.\r\nAn important item provided by {tidytext} is a dataset of stop words. Stop words are words that are very common in a language, and therefore that are irrelevant when doing text analysis. For example, “across”, “after” or “actually” do not tell anything about a text because they are too common. Also, onomatopoeia (such as “yeah” or “ugh”) do not carry much information.\r\nTo analyze the dialogues, we will remove those stop words and onomatopoeia using anti_join(). Then, we count the number of occurences and plot the results to see what words were the most used.\r\n\r\n\r\nonomatopoeia <- c(\"hey\", \"uh\", \"um\", \"yeah\", \"uhh\", \"hmm\")\r\n\r\nlist_dialogues_words[[1]] %>%\r\n  filter(!(word %in% onomatopoeia)) %>%\r\n  anti_join(stop_words, by = c(\"word\" = \"word\")) %>%\r\n  count(word, sort = TRUE) %>% \r\n  filter(n > 5) %>%\r\n  mutate(word = reorder(word, n)) %>%\r\n  ggplot(aes(word, n)) +\r\n  geom_col() +\r\n  xlab(NULL) +\r\n  coord_flip() +\r\n  geom_text(aes(label = n), nudge_y = 1) +\r\n  ggtitle(\"Most said words in episode 1 of season 1\")\r\n\r\n\r\n\r\n\r\nWe could also do this for the whole first season:\r\n\r\n\r\nlist_dialogues_words[1:13] %>%\r\n  unlist() %>%\r\n  as_tibble() %>%\r\n  rename(\"word\" = \"value\") %>%\r\n  filter(!(word %in% onomatopoeia)) %>%\r\n  anti_join(stop_words, by = c(\"word\" = \"word\")) %>%\r\n  count(word, sort = TRUE) %>%\r\n  filter(n > 50) %>%\r\n  mutate(word = reorder(word, n)) %>%\r\n  ggplot(aes(word, n)) +\r\n  geom_col() +\r\n  xlab(NULL) +\r\n  coord_flip() +\r\n  geom_text(aes(label = n), nudge_y = 10) +\r\n  ggtitle(\"Most said words in season 1\")\r\n\r\n\r\n\r\n\r\nAs we could expect, KGB, FBI and agent are in the top 15. Philip is also much more present than Elizabeth.\r\nSentiment analysis\r\nFinally, we can wonder if those words carry more positive or negative messages. Indeed, {tidytext} also provides a dataset indicating if a word is positive or negative (or neutral). Therefore, we can count the number of positive and negative words in each episode.\r\n\r\n\r\ndialogues_sentiment <- map_dfr(list_dialogues_words, function(x) {\r\n  x %>%\r\n    inner_join(tidytext::sentiments) %>%\r\n    count(sentiment)\r\n}) %>%\r\n  mutate(\r\n    season = c(rep(c(1:5), each = 26), rep(6, 10)),\r\n    episode = c(rep(rep(1:13, each = 2), times = 5), rep(1:5, each = 2))\r\n  )\r\n\r\n\r\n\r\nThen we can plot the evolution of this number during the four seasons:\r\n\r\n\r\ndialogues_sentiment %>%\r\n  ggplot(aes(x = episode, y = n, color = factor(sentiment))) + \r\n  geom_line() +\r\n  scale_x_discrete(name = \"Episode number\", limits = factor(c(1:13))) +\r\n  scale_y_continuous(name = \"Number of words\") +\r\n  ggtitle(paste0(\"Number of positive and negative words per episode and per season\")) +\r\n  facet_wrap(~ season) +\r\n  scale_color_discrete(\"Sentiment\")\r\n\r\n\r\n\r\n\r\nAnd finally, we can determine the positive and negative words that are the most said in the first season:\r\n\r\n\r\nlist_dialogues_words[1:13] %>%\r\n  unlist() %>%\r\n  as_tibble() %>%\r\n  rename(\"word\" = \"value\") %>%\r\n  inner_join(tidytext::sentiments) %>%\r\n  group_by(word) %>%\r\n  mutate(n = n()) %>%\r\n  ungroup() %>%\r\n  distinct() %>%\r\n  top_n(30) %>%\r\n  mutate(word = reorder(word, n)) %>%\r\n  ggplot(aes(word, n, fill = sentiment)) +\r\n  geom_col() +\r\n  xlab(NULL) +\r\n  coord_flip() +\r\n  facet_wrap(~ sentiment, scales = \"free_y\") +\r\n  geom_text(aes(label = n), nudge_y = 10) + \r\n  ggtitle(\"Positive and negative words most used in season 1\")\r\n\r\n\r\n\r\n\r\nOverall I think the advice of Ryan Timpe is very good: side projects (humorous or not) are very useful to learn new skills in R. To write this blog post, I had to learn how to scrape the dialogues from a website with {rvest} and how to use the {tidytext} package. I hope this was useful to you too!\r\nYou can find all the code used in this dedicated repo.\r\n\r\nSession Info\r\nThis is my session info, so that you can see the versions of packages used. This is useful if the results in my post are no longer reproducible because packages changed. The packages with a star (*) are those explicitely called in the script.\r\n\r\n─ Session info ─────────────────────────────────────────────────────\r\n setting  value                       \r\n version  R version 4.0.3 (2020-10-10)\r\n os       Ubuntu 18.04.5 LTS          \r\n system   x86_64, linux-gnu           \r\n ui       X11                         \r\n language en                          \r\n collate  fr_FR.UTF-8                 \r\n ctype    fr_FR.UTF-8                 \r\n tz       Europe/Paris                \r\n date     2021-01-20                  \r\n\r\n─ Packages ─────────────────────────────────────────────────────────\r\n package     * version date       lib\r\n assertthat    0.2.1   2019-03-21 [1]\r\n backports     1.2.1   2020-12-09 [1]\r\n broom         0.7.3   2020-12-16 [1]\r\n cellranger    1.1.0   2016-07-27 [1]\r\n cli           2.2.0   2020-11-20 [1]\r\n colorspace    2.0-0   2020-11-11 [1]\r\n crayon        1.3.4   2017-09-16 [1]\r\n curl          4.3     2019-12-02 [1]\r\n DBI           1.1.1   2021-01-15 [1]\r\n dbplyr        2.0.0   2020-11-03 [1]\r\n digest        0.6.27  2020-10-24 [1]\r\n distill       1.2     2021-01-13 [1]\r\n downlit       0.2.1   2020-11-04 [1]\r\n dplyr       * 1.0.3   2021-01-15 [1]\r\n ellipsis      0.3.1   2020-05-15 [1]\r\n evaluate      0.14    2019-05-28 [1]\r\n fansi         0.4.2   2021-01-15 [1]\r\n farver        2.0.3   2020-01-16 [1]\r\n forcats     * 0.5.0   2020-03-01 [1]\r\n fs            1.5.0   2020-07-31 [1]\r\n generics      0.1.0   2020-10-31 [1]\r\n ggplot2     * 3.3.3   2020-12-30 [1]\r\n glue          1.4.2   2020-08-27 [1]\r\n gtable        0.3.0   2019-03-25 [1]\r\n haven         2.3.1   2020-06-01 [1]\r\n hms           1.0.0   2021-01-13 [1]\r\n htmltools     0.5.1   2021-01-12 [1]\r\n httr          1.4.2   2020-07-20 [1]\r\n janeaustenr   0.1.5   2017-06-10 [1]\r\n jsonlite      1.7.2   2020-12-09 [1]\r\n knitr         1.30    2020-09-22 [1]\r\n labeling      0.4.2   2020-10-20 [1]\r\n lattice       0.20-41 2020-04-02 [4]\r\n lifecycle     0.2.0   2020-03-06 [1]\r\n lubridate     1.7.9.2 2020-11-13 [1]\r\n magrittr      2.0.1   2020-11-17 [1]\r\n Matrix        1.2-18  2019-11-27 [4]\r\n modelr        0.1.8   2020-05-19 [1]\r\n munsell       0.5.0   2018-06-12 [1]\r\n pillar        1.4.7   2020-11-20 [1]\r\n pkgconfig     2.0.3   2019-09-22 [1]\r\n purrr       * 0.3.4   2020-04-17 [1]\r\n R6            2.5.0   2020-10-28 [1]\r\n Rcpp          1.0.6   2021-01-15 [1]\r\n readr       * 1.4.0   2020-10-05 [1]\r\n readxl        1.3.1   2019-03-13 [1]\r\n reprex        0.3.0   2019-05-16 [1]\r\n rlang         0.4.10  2020-12-30 [1]\r\n rmarkdown     2.6.4   2021-01-19 [1]\r\n rstudioapi    0.13    2020-11-12 [1]\r\n rvest       * 0.3.6   2020-07-25 [1]\r\n scales        1.1.1   2020-05-11 [1]\r\n selectr       0.4-2   2019-11-20 [1]\r\n sessioninfo   1.1.1   2018-11-05 [1]\r\n SnowballC     0.7.0   2020-04-01 [1]\r\n stringi       1.5.3   2020-09-09 [1]\r\n stringr     * 1.4.0   2019-02-10 [1]\r\n tibble      * 3.0.5   2021-01-15 [1]\r\n tidyr       * 1.1.2   2020-08-27 [1]\r\n tidyselect    1.1.0   2020-05-11 [1]\r\n tidytext    * 0.3.0   2021-01-06 [1]\r\n tidyverse   * 1.3.0   2019-11-21 [1]\r\n tokenizers    0.2.1   2018-03-29 [1]\r\n vctrs         0.3.6   2020-12-17 [1]\r\n withr         2.4.0   2021-01-16 [1]\r\n xfun          0.20    2021-01-06 [1]\r\n xml2        * 1.3.2   2020-04-23 [1]\r\n yaml          2.2.1   2020-02-01 [1]\r\n source                            \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.2)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.2)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.2)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.3)                    \r\n Github (rstudio/rmarkdown@2e8572e)\r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.2)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.2)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.2)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.2)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.0)                    \r\n\r\n[1] /home/etienne/R/x86_64-pc-linux-gnu-library/4.0\r\n[2] /usr/local/lib/R/site-library\r\n[3] /usr/lib/R/site-library\r\n[4] /usr/lib/R/library\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-10-04-the-americans/distill-preview.png",
    "last_modified": "2022-05-03T17:54:55+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-07-16-tips-and-tricks-r-markdown/",
    "title": "Writing a Master's thesis with R Markdown and Bookdown",
    "description": "I wrote my Master's thesis with bookdown. This post contains some tips to modify the layout and other stuff.",
    "author": [
      {
        "name": "Etienne Bacher",
        "url": {}
      }
    ],
    "date": "2020-07-16",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nWhy bookdown?\r\nLaTeX packages and commands\r\nTitlepage, acknowledgements and abstract\r\nInclude LaTeX files in YAML\r\nSummary\r\n\r\nI finished my Master’s thesis very recently, and I wrote it with R Markdown, and more precisely with the bookdown package. It was really comfortable to do absolutely everything with R: data treatment, use of econometric methods, redaction with chunk of codes, and even the slides for the presentation! However, I have also spent a non-negligible part of my time trying to have a correct layout, essentially for the first pages. I found that some things were not as easy to do as they should be. This post contains some solutions to these problems.\r\nWhy bookdown?\r\nA small preamble before starting to list these problems and solutions: why did I use bookdown and not rmarkdown? bookdown has a few advantages that are very important when writing a Master’s thesis (or an academic paper in general), such as cross-references between sections, figures, tables, etc. See the bookdown book for all the details.\r\nWhile it is possible to create a bookdown project with RStudio, I “manually” made my own, because I have the impression there are a lot of files created automatically by RStudio that would just confuse myself. Therefore, I had a file containing the YAML and the chunks necessary to load all the packages needed and to run the child documents. Child documents are .Rmd files that contain only some Markdown text and code chunks (no YAML). They make it much easier to write a thesis since it is possible to divide it in several pieces (introduction, literature review, method, etc.).\r\nIn addition to Global.Rmd (which contains YAML and setup chunks), I used two .tex files: preamble.tex is where I put all the LaTeX commands and packages I used, and titlepage.tex to make my custom titlepage, acknowledgments and abstract before the table of contents.\r\nLaTeX packages and commands\r\nIn preamble.tex, I put all the LaTeX commands, many of them being \\usepackage. For example, the R package kableExtra provides a list of LaTeX packages required to be able to customize the tables (see here).\r\nHere’s a small list of the commands I used for my thesis.\r\n\\renewcommand{\\baselinestretch}{1.3} to change the space between lines;\r\n\\pagenumbering{gobble} to remove page numbering for the first pages, that contain the title, the table of content (TOC), the lists of figures and tables (LOF and LOT) and some blank pages. I used \\pagenumbering{arabic} after the YAML to start the page numbering at the right place.\r\n\\usepackage{caption}; \\captionsetup[table]{name=Tableau}; \\captionsetup[figure]{name=Figure}. These three commands (to put on three separate lines) are here to change the name of tables and figures. If you’re writing in English, you probably won’t need them, but they were necessary to write in French.\r\nthe following lines create a new command to create a blank page after the titlepage. Strangely, \\pagebreak or \\newpage didn’t work inside the titlepage and I had to find an alternative:\r\n\r\nCommand to create a blank page\r\n\r\n\\usepackage{afterpage}\r\n\\newcommand\\blankpage{%\r\n    \\null\r\n    \\thispagestyle{empty}%\r\n    \\addtocounter{page}{-1}%\r\n    \\newpage}\r\n\r\nTitlepage, acknowledgements and abstract\r\nConcerning these three components, I put them in another file named titlepage.tex. This is the layout I wanted:\r\na titlepage with some logos and some information on my thesis and my university;\r\na blank page;\r\nacknowledgements on a new page;\r\nabstract on a new page;\r\nTOC, LOF and LOT on a new page.\r\nTo do so, I started the titlepage with \\begin{titlepage} and customized it as I wanted. But before putting \\end{titlepage}, I placed \\afterpage{\\blankpage}, which is the command we define in preamble.tex. With this, I had a titlepage and a blank page.\r\nThe next step was to create two pages containing the acknowledgements and the abstract This was easily done in LaTeX, and this time I could use \\pagebreak at the end of the acknowledgements to create a new page for the abstract. I also put another \\pagebreak to finish titlepage.tex, so that TOC, LOF and LOT (created in the YAML) could start on a new page.\r\nInclude LaTeX files in YAML\r\nAs explained, I had two .tex files to run when compiling the R Markdown file:\r\npreamble.tex is a list of commands that should be placed before \\begin{document} when this .Rmd file will be converted to .tex. Therefore, I used in_header in the YAML to compile it.\r\ntitlepage.tex contains some elements that should be in the final PDF document. Since this should appear first (before the rest of the .Rmd document), I used before_body in the YAML.\r\nSummary\r\nHere are examples of the three files: Global.Rmd, preamble.tex and titlepage.tex.\r\nGlobal.Rmd\r\n\r\n---\r\noutput: \r\n  bookdown::pdf_book:\r\n    includes:\r\n      in_header: preamble.tex\r\n      before_body: titlepage.tex\r\n    keep_tex: true\r\n    toc: yes\r\n    toc_depth: 3\r\nindent: true\r\nlink-citations: yes\r\nlot: true\r\nlof: true\r\n---\r\n\r\n<!-- Start the redaction on a new page -->\r\n\\newpage\r\n\r\n<!-- Start page numbering where the redaction starts -->\r\n\\pagenumbering{arabic}\r\n\r\n```{r globaloptions, include=FALSE}\r\n# Include here chunk options\r\n```\r\n\r\n```{r packages}\r\n# Load here the packages\r\n```\r\n\r\n<!-- Call the child documents -->\r\n```{r body, child = c('01-Intro.Rmd', '02-Literature.Rmd', '03-Data-and-method.Rmd', '04-Results.Rmd', '05-Discussion.Rmd', '06-Conclusion.Rmd')}\r\n```\r\n\r\n<!-- Placement of bibliography -->\r\n# References {-}\r\n\r\n<div id=\"refs\"><\/div>\r\n\r\n\r\n<!-- Place the appendix after the bibliography -->\r\n```{r appendix, child = c('07--Appendix.Rmd')}\r\n```\r\n\r\npreamble.tex\r\n\r\n% Line spacing\r\n\\renewcommand{\\baselinestretch}{1.3}\r\n\r\n% Page number and chapter at the top of the page\r\n\\pagestyle{headings}\r\n\r\n% Important packages\r\n\\usepackage[utf8]{inputenc}\r\n\\usepackage[T1]{fontenc}\r\n\\usepackage[dvipsnames]{xcolor}\r\n\\usepackage{hyperref}\r\n\\usepackage{dcolumn}\r\n\\usepackage{booktabs}\r\n\\usepackage{longtable}\r\n\r\n% Figure and table names \r\n\\usepackage{caption}\r\n\\captionsetup[table]{name=Tableau} \r\n\\captionsetup[figure]{name=Figure}\r\n\r\n% Packages for kableExtra\r\n\\usepackage{array}\r\n\\usepackage{multirow}\r\n\\usepackage{wrapfig}\r\n\\usepackage{colortbl}\r\n\\usepackage{pdflscape}\r\n\\usepackage{float}\r\n\\usepackage{tabu}\r\n\\usepackage{threeparttable}\r\n\\usepackage{threeparttablex}\r\n\\usepackage[normalem]{ulem}\r\n\\usepackage{makecell}\r\n\r\n% Remove page numbering before start of redaction\r\n\\pagenumbering{gobble}\r\n\r\n% Command to make a blank page\r\n\\usepackage{afterpage}\r\n\\newcommand\\blankpage{%\r\n    \\null\r\n    \\thispagestyle{empty}%\r\n    \\addtocounter{page}{-1}%\r\n    \\newpage}\r\n\r\ntitlepage.tex\r\n\r\n\\begin{titlepage}\r\n\\centering\r\nTitle of my thesis\r\n\\afterpage{\\blankpage}\r\n\\end{titlepage}\r\n\r\n\\section*{Acknowledgements}\r\nThanks everyone\r\n\\pagebreak\r\n\r\n\\begin{center}\r\n\\textbf{Abstract}\r\n\\end{center}\r\nBla bla bla...\r\n\\pagebreak\r\n\r\nHope this helps!\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-03T17:54:55+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-05-22-code-doesnt-work/",
    "title": "What to do when your code doesn't work?",
    "description": "This is the list of actions I make when I have an error in R.",
    "author": [
      {
        "name": "Etienne Bacher",
        "url": {}
      }
    ],
    "date": "2020-05-22",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nRead the error message\r\nSearch on StackOverflow and RStudio Community\r\nSearch on Github\r\nPost my question online - Part 1\r\nPost my question online - Part 2\r\nAdditional resources\r\n\r\nAs Jenny Bryan said, one of the first things people do when there is a problem with the code is run it a second time. But in 95% of the cases (in my small programming experience), this does not solve the problem, and I realized I developed a small pattern of reactions when I am in this situation.\r\nRead the error message\r\nThis may seem obvious since this is the first thing we see when there is an error. However, some messages (like those in the tidyverse packages) are really helpful and are enough to solve the problem.\r\nAs you spend time programming, some error messages become quite familiar over time. You don’t necessarily need to fully understand the message, just remember how you solved the error when you saw this. But this does not help if this is the first time you see the message.\r\nSearch on StackOverflow and RStudio Community\r\nIf the error message was not helpful, I quickly go to step 2: search online (which is almost always equivalent to “search on StackOverflow and RStudio Community”). Those are the two places where you have the most odds of finding an answer to your problem. If your problem concerns mainstream packages (once again, such as the tidyverse ones), then there will always be at least one person who had a similar problem for which a solution was given.\r\nSearch on Github\r\nIf no answer was available on these two websites, I usually go a step further and search about it on GitHub, especially if the problem comes from an unknown and/or new package. Indeed, the source code of a lot of R packages is available on GitHub, as well as the brand-new versions (not on CRAN yet). It is also the place where people make feedback on some issues or help develop the package.\r\nIt happens that somebody reported a similar issue and that the package maintainer answered. If not, well, it is always useful to see the situations in which people use the package, or to learn new functions of this package.\r\nPost my question online - Part 1\r\nI have an error, I don’t understand the error message and there are no answers online. What can I do now?\r\nYou can post your question on one of the websites I mentioned. My order of preference is:\r\nStackOverflow: in general, more people see your message and therefore more people might answer your question.\r\nRStudio Community: more “specialized” and some mainstream packages maintainers are there. Also useful when the question is about RStudio IDE (not to be confused with the R language) since they developed it.\r\nGitHub: clearly the problem comes from an error specific to the package and I couldn’t find an answer online. Most of the time, there is no need to create new issues.\r\nHowever: posting a question online implies that you need to create a reproducible example. What is a reproducible example?\r\nSuppose that you are working on your data and have a problem. You may want to copy and paste your code online. It’s quick and easy. But try to imagine you’re someone on StackOverflow that sees your code for the first time. This person needs to run it to understand what your code does and what the problem is. But that’s impossible, because you load CSV data that is only available for you, or because you forgot to mention the packages you’re using. Therefore, no one can run your code and the odds of someone solving your problem plummet.\r\nIf you post online, your post MUST contain:\r\nA description of what you’re trying to do and of your problem in plain text.\r\nSome code that contains:\r\nthe library() calls needed to run every function you use (but no need to put packages that you never use in your example)\r\na code that can be copied and pasted in a new R session and that ends up in the same situation as you’re in. That means that the data you use must be reproducible.\r\nthe expected output, if possible. If you can easily show the output you want, do it. It will be easier to help you if you show what you want. In some cases (e.g Shiny apps), it is not always possible or easy to provide this, so it is not a 100% necessary.\r\n\r\nMaking a reproducible example takes time but is incredibly helpful. Open a new tab in RStudio, try to simplify your situation as much as possible and to make it reproducible by using data available for everyone. For example, some datasets are automatically in R, such as mtcars or iris. Don’t forget to mention the packages you use in library(). Once you think you have finished (you show what packages you use, you keep your example as small as possible and you show the expected output), restart the session (ctrl + shift + F10) and see if it runs and if it reproduces the same error. If it doesn’t, it means your example is not reproducible and that you have to improve it.\r\nI would say that I solve the problem myself by making a reproducible example in 70% of the cases, so taking the time to make one is worth it!\r\nPost my question online - Part 2\r\nHowever, if you didn’t solve your problem with this (and if your example is reproducible), you can post it on StackOverflow or RStudio Community. Don’t forget to explain your situation and your code, don’t paste it without any details!\r\nHopefully, you’ll have an answer.\r\nAdditional resources\r\nHere are two pages with details about making a reproducible example:\r\nhttps://stackoverflow.com/help/minimal-reproducible-example\r\nhttps://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-03T17:54:55+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-04-08-my-shiny-app/",
    "title": "My application for the Shiny Contest (2020)",
    "description": "A presentation of the application I submitted to the Shiny Contest 2nd edition.",
    "author": [
      {
        "name": "Etienne Bacher",
        "url": {}
      }
    ],
    "date": "2020-04-08",
    "categories": [],
    "contents": "\r\nOne of the greatest things about R is the possibility to build websites quite easily with R Shiny. I started to create apps with Shiny almost immediately after having discovered it. This was in May last year and two months later I thought it would be a great idea to build an app to treat more easily World Bank data. Indeed, in my field, World Development Indicators (WDI) are often used and I thought it would be useful to have a graphical interface where we can import and treat these indicators.\r\nSo in July last year I started to work on this. Besides building something useful, I considered this as a good opportunity to practice with R Shiny. I worked on that irregularly during the rest of the year. Sometimes I considerably improved my app in a week and sometimes I spent two months not thinking about it.\r\nIn January, I was quite stuck: my app required to generate pieces of UI (User Interface) on the fly and I didn’t know how to do that. But when I saw the announcement for the second edition of the Shiny Contest, I convinced myself to give it a try and worked a lot on my app, especially since I finally understood how to use modules. Therefore, after a few weeks, I could finally deploy my app and participate to the contest. Given the incredible apps of other participants (just look at the “Shiny Contest” tag on RStudio Community), I know that there is not a chance that I win something but nonetheless I am very proud of having the possibility to show what I can do.\r\n“That’s very good”, you may say, “but what does your app do?”. Well if you are familiar with the WDIs, you know that each indicator has an ID (like “NV.AGR.TOTL.ZS”). Using this ID in my app, you can import the dataset related, choose the type of data you want (cross-country, time series, panel data), the country/countries and year(s) you want and compute the logarithm, the squared value and the lagged value of the variable. You may also generate a plot that you can download for this dataset. You can import and manipulate as many datasets as you desire and when you are done, you can merge them in a final dataset that is also downloadable. Finally, since reproducibility is a big aspect in science, all the manipulations you did are translated into R code so that you can copy and paste this code in a fresh R session and it will reproduce everything you have done. This was made possible thanks the great (but still experimental) shinymeta package1.\r\nYou can try the app here, but if it has been removed by the time you go checking it, you may find the source code for the whole app on GitHub.\r\n\r\nLuckily, I have discovered this package a month before I launched my app, in a rstudio::conf 2020 presentation by Carson Sievert↩︎\r\n",
    "preview": {},
    "last_modified": "2022-05-03T17:54:55+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-04-06-my-favorite-shortcuts/",
    "title": "My favorite shortcuts in RStudio",
    "description": "Did you know that RStudio contains a lot of shortcuts to write code faster? Here are my favorites.",
    "author": [
      {
        "name": "Etienne Bacher",
        "url": {}
      }
    ],
    "date": "2020-04-06",
    "categories": [],
    "contents": "\r\n\r\nContents\r\n“Ctrl+Shift+Enter”: run the whole script\r\n“Ctrl+Shift+F10”: restart R session\r\n“Ctrl+Shift+K”: knit Markdown documents\r\n“Ctrl+Alt+B”: run the script from the beginning to where I am\r\n“Ctrl+Shift+A”: good alignment\r\n“Ctrl+Shift+R”: new section label\r\n“Ctrl+Shift+N”: new file\r\n“Ctrl+Shift+C”: comment and un-comment lines of code\r\n\r\nCurrently working on my master’s thesis, I spend almost half on my day on R (the other half being spent reading articles). I have learnt a few very useful shortcuts by reading blog posts or free R books here and there and I have decided to make a small list of those I use most.\r\n“Ctrl+Shift+Enter”: run the whole script\r\nThis combination will run all of your script, wherever the mouse is in the script. This considerably saves some time and is much more convenient than having to drag the mouse up to the “Run” button.\r\n“Ctrl+Shift+F10”: restart R session\r\nAlmost all your work will have to be done again one day, either by you or by somebody else. That day, you will regret not having saved all the packages you needed in your R script. How is that possible? Well, it comes from the fact that I (and presumably other people too) load some packages and their dependencies in the same session because we test some packages, or because we are trying to solve a StackOverflow problem during a break, etc. Therefore, when you were writing it, some of your code was running only thanks to some packages you didn’t keep in your script.\r\nTo prevent this to happen again, you can run “Ctrl+Shift+F10” sometimes: it restarts the R session (but does not close RStudio) and allows to see quickly if your code can run all by itself. When I have finished writing a chunk of code, I always do “Ctrl+Shift+F10” and “Ctrl+Shift+Enter” to make sure that my code can run again in three months or more.\r\n“Ctrl+Shift+K”: knit Markdown documents\r\nI do not use this everywhere but only in Markdown documents: it knits the document. As “Ctrl+Shift+Enter” replaces the “Run” button, “Ctrl+Shift+K” replaces the “Knit” button.\r\n“Ctrl+Alt+B”: run the script from the beginning to where I am\r\nThis is similar to “Ctrl+Shift+Enter” but it runs the script only until your mouse.\r\n“Ctrl+Shift+A”: good alignment\r\nIndentation is quite important to have a code that is readable, especially when you begin to write more than 10 lines of code. Select your code (or just a piece of it) and do this combination to apply indentation rules automatically.\r\n“Ctrl+Shift+R”: new section label\r\nThis creates a new section in your code, with the title you choose. Quite convenient to keep a readable code.\r\n“Ctrl+Shift+N”: new file\r\nIf you want to try a chunk of code quickly, you can run this combination to open a new file in RStudio.\r\n“Ctrl+Shift+C”: comment and un-comment lines of code\r\nTo me, this is one of the most convenient shortcut. It simply allows to comment (and un-comment) at once all the lines selected.\r\n\r\nThe complete list of shortcuts is available in “Tools -> Keyboard Shortcuts Help” in RStudio. Other useful tools:\r\nsnippets\r\naddins\r\nRStudio tips (on Twitter)\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-03T17:54:55+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-01-22-first-contact/",
    "title": "First contact with the data on R",
    "description": "A blog post describing the first steps of data cleaning and analysis using R.",
    "author": [
      {
        "name": "Etienne Bacher",
        "url": {}
      }
    ],
    "date": "2020-01-22",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nFiles used and organization of the project\r\nImport data\r\nMerge dataframes\r\nClean the data\r\nDescriptive Statistics\r\nPlots\r\n\r\n\r\n Note: \r\n\r\n\r\nIn this and future articles, you will see some arrows below R code. If you click on it, it will display the Stata code equivalent to the R code displayed. However, since those are two different softwares, they are not completely equivalent and some of the Stata code may not fully correspond to the R code. Consider it more like a reference point not to be lost rather than like an exact equivalent.\r\n\r\n\r\nIn this post, you will see how to import and treat data, make descriptive statistics and a few plots. I will also show you a personal method to organize one’s work.\r\nFiles used and organization of the project\r\nFirst of all, you need to create a project. In RStudio, you can do “File”, “New Project” and then choose the location of the project and its name. In the folder that contains the project, I have several sub-folders: Figures, Bases_Used, Bases_Created. To be able to save or use files in these particular sub-folders, I use the package here. The command here() shows the path to your project and you just need to complete the path to access to your datasets or other files.\r\n\r\n\r\n# if you've never installed this package before, do:\r\n# install.packages(\"here\")\r\nlibrary(here)\r\n\r\n\r\n\r\nWhy is this package important? Your code must be reproducible, either for your current collaborators to work efficiently with you or for other people to check your code and to use it in the future. Using paths that work only for your computer (like “/home/Mr X/somefolder/somesubfolder/Project”) makes it longer and more annoying to use your code since it requires to manually change paths in order to import data or other files. The package here makes it much easier to reproduce your code since it automatically detects the path to access to your data. You only need to keep the same structure between R files and datasets. You will see in the next part how to use it.\r\nImport data\r\nWe will use data contained in Excel (.xlsx) and text (.txt) files. You can find these files (and the full R script corresponding to this post) here. To import Excel data, we will need the readxl package.\r\n\r\n\r\nlibrary(readxl)\r\n\r\n\r\n\r\nWe use the read_excel function of this package to import excel files and the function read.table (in base R) to import the data:\r\n\r\n\r\n\r\nbase1 <- read_excel(here(\"Bases_Used/Base_Excel.xlsx\"), sheet = \"Base1\")\r\nbase2 <- read_excel(here(\"Bases_Used/Base_Excel.xlsx\"), sheet = \"Base2\")\r\nbase3 <- read_excel(here(\"Bases_Used/Base_Excel.xlsx\"), sheet = \"Base3\")\r\nbase4 <- read.table(here(\"Bases_Used/Base_Text.txt\"), header = TRUE)\r\n\r\n\r\n\r\n\r\n\r\nStata\r\n\r\ncd \"/path/to/Bases_Used\"\r\nimport excel using Base_Excel, sheet(\"Base1\") firstrow\r\nAs you can see, if your project is in a folder and if you stored you datasets in the Bases_Used subfolder, this code will work automatically since here detects the path. Now, we have stored the four datasets in four objects called data.frames. To me, this simple thing is an advantage on Stata where storing multiple datasets in the same time is not intuitive at all.\r\nMerge dataframes\r\nWe want to have a unique dataset to make descriptive statistics and econometrics (we will just do descriptive statistics in this post). Therefore, we will merge these datasets together, first by using the dplyr package. This package is one of the references for data manipulation. It is extremely useful and much more easy to use than base R. You may find a cheatsheet (i.e. a recap of the functions) for this package here, along with cheatsheets of many other great packages.\r\nFirst, we want to regroup base1 and base2. To do so, we just need to put one under the other and to “stick” them together with bind_rows and we observe the result:\r\n\r\n\r\nlibrary(dplyr)\r\nbase_created <- bind_rows(base1, base2)\r\nbase_created\r\n\r\n\r\n# A tibble: 23 x 6\r\n    hhid indidy1 surname   name     gender  wage\r\n   <dbl>   <dbl> <chr>     <chr>     <dbl> <dbl>\r\n 1     1       1 BROWN     Robert        1  2000\r\n 2     1       2 JONES     Michael       1  2100\r\n 3     1       3 MILLER    William       1  2300\r\n 4     1       4 DAVIS     David         1  1800\r\n 5     2       1 RODRIGUEZ Mary          2  3600\r\n 6     2       2 MARTINEZ  Patricia      2  3500\r\n 7     2       3 WILSON    Linda         2  1900\r\n 8     2       4 ANDERSON  Richard       1  1900\r\n 9     3       1 THOMAS    Charles       1  1800\r\n10     3       2 TAYLOR    Barbara       2  1890\r\n# … with 13 more rows\r\n\r\n\r\nStata\r\n\r\npreserve\r\n\r\n*** Open base #2 and bind the rows\r\nclear all \r\nimport excel using Base_Excel, sheet(\"Base2\") firstrow\r\ntempfile base2\r\nsave  `base2' \r\nrestore\r\nappend using `base2'\r\nAs you can see, we obtain a dataframe with 6 columns (like each table separately) and 23 rows: 18 in the first table, 5 in the second table. Now, we merge this dataframe with base3. base_created and base3 only have one column in common (hhid) so we will need to specify that we want to merge these two bases by this column:\r\n\r\n\r\nbase_created <- left_join(base_created, base3, by = \"hhid\")\r\nbase_created\r\n\r\n\r\n# A tibble: 23 x 7\r\n    hhid indidy1 surname   name     gender  wage location\r\n   <dbl>   <dbl> <chr>     <chr>     <dbl> <dbl> <chr>   \r\n 1     1       1 BROWN     Robert        1  2000 France  \r\n 2     1       2 JONES     Michael       1  2100 France  \r\n 3     1       3 MILLER    William       1  2300 France  \r\n 4     1       4 DAVIS     David         1  1800 France  \r\n 5     2       1 RODRIGUEZ Mary          2  3600 England \r\n 6     2       2 MARTINEZ  Patricia      2  3500 England \r\n 7     2       3 WILSON    Linda         2  1900 England \r\n 8     2       4 ANDERSON  Richard       1  1900 England \r\n 9     3       1 THOMAS    Charles       1  1800 Spain   \r\n10     3       2 TAYLOR    Barbara       2  1890 Spain   \r\n# … with 13 more rows\r\n\r\n\r\nStata\r\n\r\npreserve \r\n\r\n*** Open base #3 and merge\r\nclear all\r\ncd ..\\Bases_Used \r\nimport excel using Base_Excel, sheet(\"Base3\") firstrow\r\ntempfile base3\r\nsave `base3'\r\nrestore \r\nmerge m:1 hhid using `base3' \r\ndrop _merge \r\nleft_join is a dplyr function saying that the first dataframe mentioned (here base_created) is the “most important” and that we will stick the second one (here base3) to it. If there are more rows in the first one than in the second one, then there will be some missing values but the number of rows will stay the same. If we knew that base3 had more rows than base_created, we would have used right_join.\r\nWe now want to merge base_created with base4. The problem is that there are no common columns so we will need to create one in each. Moreover, base_created contains data for the year 2019 and base4 for the year 2020. We will need to create columns to specify that too:\r\n\r\n\r\n# rename the second column of base_created and of base4\r\ncolnames(base_created)[2] <- \"indid\"\r\ncolnames(base4)[2] <- \"indid\"\r\n\r\n# create the column \"year\", that will take the value 2019 \r\n# for base_created and 2020 for base4\r\nbase_created$year <- 2019\r\nbase4$year <- 2020\r\n\r\n\r\n\r\nFrom this point, we can merge these two dataframes:\r\n\r\n\r\nbase_created2 <- bind_rows(base_created, base4)\r\nbase_created2\r\n\r\n\r\n# A tibble: 46 x 8\r\n    hhid indid surname   name     gender  wage location  year\r\n   <dbl> <dbl> <chr>     <chr>     <dbl> <dbl> <chr>    <dbl>\r\n 1     1     1 BROWN     Robert        1  2000 France    2019\r\n 2     1     2 JONES     Michael       1  2100 France    2019\r\n 3     1     3 MILLER    William       1  2300 France    2019\r\n 4     1     4 DAVIS     David         1  1800 France    2019\r\n 5     2     1 RODRIGUEZ Mary          2  3600 England   2019\r\n 6     2     2 MARTINEZ  Patricia      2  3500 England   2019\r\n 7     2     3 WILSON    Linda         2  1900 England   2019\r\n 8     2     4 ANDERSON  Richard       1  1900 England   2019\r\n 9     3     1 THOMAS    Charles       1  1800 Spain     2019\r\n10     3     2 TAYLOR    Barbara       2  1890 Spain     2019\r\n# … with 36 more rows\r\n\r\n\r\nStata\r\n\r\n\r\nrename indidy1 indid \r\ngen year=2019 \r\npreserve \r\n\r\n* Open base #4 and merge\r\nclear all\r\nimport delimited Base_Text.txt \r\nrename indidy2 indid \r\ngen year=2020\r\ntempfile base4\r\nsave `base4'\r\nrestore \r\n\r\nmerge 1:1 hhid indid year using `base4'\r\ndrop _merge\r\nBut we have many missing values for the new rows because base4 only contained three columns. We want to have a data frame arranged by household then by individual and finally by year. Using only dplyr functions, we can do:\r\n\r\n\r\nbase_created2 <- base_created2 %>% \r\n  group_by(hhid, indid) %>% \r\n  arrange(hhid, indid, year) %>%\r\n  ungroup()\r\nbase_created2\r\n\r\n\r\n# A tibble: 46 x 8\r\n    hhid indid surname   name    gender  wage location  year\r\n   <dbl> <dbl> <chr>     <chr>    <dbl> <dbl> <chr>    <dbl>\r\n 1     1     1 BROWN     Robert       1  2000 France    2019\r\n 2     1     1 <NA>      <NA>        NA  2136 <NA>      2020\r\n 3     1     2 JONES     Michael      1  2100 France    2019\r\n 4     1     2 <NA>      <NA>        NA  2362 <NA>      2020\r\n 5     1     3 MILLER    William      1  2300 France    2019\r\n 6     1     3 <NA>      <NA>        NA  2384 <NA>      2020\r\n 7     1     4 DAVIS     David        1  1800 France    2019\r\n 8     1     4 <NA>      <NA>        NA  2090 <NA>      2020\r\n 9     2     1 RODRIGUEZ Mary         2  3600 England   2019\r\n10     2     1 <NA>      <NA>        NA  3784 <NA>      2020\r\n# … with 36 more rows\r\n\r\nNotice that there are some %>% between the lines: it is a pipe and its function is to connect lines of code between them so that we don’t have to write base_created2 every time. Now that our dataframe is arranged, we need to fill the missing values. Fortunately, these missing values do not change for an individual since they concern the gender, the location, the name and the surname. So basically, we can just take the value of the cell above (corresponding to year 2019) and replicate it in each cell (corresponding to year 2020):\r\n\r\n\r\nlibrary(tidyr)\r\nbase_created2 <- base_created2 %>%\r\n  fill(select_if(., ~ any(is.na(.))) %>% \r\n         names(),\r\n       .direction = 'down')\r\n\r\n\r\n\r\n\r\nStata\r\n\r\nforeach x of varlist surname name gender location {\r\n  bysort hhid indid: replace `x'=`x'[_n-1] if year==2020\r\n}\r\nLet me explain the code above:\r\nfill aims to fill cells\r\nselect_if selects columns according to the condition defined\r\nany(is.na(.)) is a logical question asking if there are missing values (NA)\r\n. indicates that we want to apply the function to the whole dataframe\r\nnames tells us what the names of the columns selected are\r\n.direction tells the direction in which the filling goes\r\nSo fill(select_if(., ~ any(is.na(.))) %>% names(), .direction = 'down') means that for the dataframe, we select each column which has some NA in it and we obtain their names. In these columns, the empty cells are filled by the value of the cell above (since the direction is “down”).\r\nFinally, we want the first three columns to be hhid, indid and year, and we create a ID column named hhind which is just the union of hhid and indid.\r\n\r\n\r\nbase_created2 <- base_created2 %>%\r\n  select(hhid, indid, year, everything()) %>%\r\n  unite(hhind, c(hhid, indid), sep = \"\", remove = FALSE) \r\nbase_created2\r\n\r\n\r\n# A tibble: 46 x 9\r\n   hhind  hhid indid  year surname   name    gender  wage location\r\n   <chr> <dbl> <dbl> <dbl> <chr>     <chr>    <dbl> <dbl> <chr>   \r\n 1 11        1     1  2019 BROWN     Robert       1  2000 France  \r\n 2 11        1     1  2020 BROWN     Robert       1  2136 France  \r\n 3 12        1     2  2019 JONES     Michael      1  2100 France  \r\n 4 12        1     2  2020 JONES     Michael      1  2362 France  \r\n 5 13        1     3  2019 MILLER    William      1  2300 France  \r\n 6 13        1     3  2020 MILLER    William      1  2384 France  \r\n 7 14        1     4  2019 DAVIS     David        1  1800 France  \r\n 8 14        1     4  2020 DAVIS     David        1  2090 France  \r\n 9 21        2     1  2019 RODRIGUEZ Mary         2  3600 England \r\n10 21        2     1  2020 RODRIGUEZ Mary         2  3784 England \r\n# … with 36 more rows\r\n\r\n\r\nStata\r\n\r\negen hhind=group(hhid indid) \r\norder hhind hhid indid year * \r\nsort hhid indid year \r\nThat’s it, we now have the complete dataframe.\r\nClean the data\r\nThere are still some things to do. First, we remark that there are some errors in the column location (England_error and Spain_error) so we correct it:\r\n\r\n\r\n# display the unique values of the column \"location\"\r\nunique(base_created2$location)\r\n\r\n\r\n[1] \"France\"        \"England\"       \"Spain\"         \"Italy\"        \r\n[5] \"England_error\" \"Spain_error\"  \r\n\r\n# correct the errors\r\nbase_created2[base_created2 == \"England_error\"] <- \"England\"\r\nbase_created2[base_created2 == \"Spain_error\"] <- \"Spain\"\r\nunique(base_created2$location)\r\n\r\n\r\n[1] \"France\"  \"England\" \"Spain\"   \"Italy\"  \r\n\r\n\r\nStata\r\n\r\nreplace localisation=\"England\" if localisation==\"England_error\"\r\nreplace localisation=\"Spain\" if localisation==\"Spain_error\"\r\nBasically, what we’ve done here is that we have selected every cell in the whole dataframe that had the value England_error (respectively Spain_error) and we replaced these cells by England (Spain). We also need to recode the column gender because binary variables have to take values of 0 or 1, not 1 or 2.\r\n\r\n\r\nbase_created2$gender <- recode(base_created2$gender, `2` = 0)\r\n\r\n\r\n\r\n\r\nStata\r\n\r\nlabel define genderlab 1 \"M\" 2 \"F\"\r\nlabel values gender genderlab\r\nrecode gender (2=0 \"Female\") (1=1 \"Male\"), gen(gender2)\r\ndrop gender\r\nrename gender2 gender\r\nTo have more details on the dataframe, we need to create some labels. To do so, we need the upData function in the Hmisc package.\r\n\r\n\r\nlibrary(Hmisc)\r\nvar.labels <- c(hhind = \"individual's ID\",\r\n                hhid = \"household's ID\",\r\n                indid = \"individual's ID in the household\",\r\n                year = \"year\",\r\n                surname = \"surname\",\r\n                name = \"name\",\r\n                gender = \"1 if male, 0 if female\",\r\n                wage = \"wage\",\r\n                location = \"household's location\")\r\nbase_created2 <- upData(base_created2, labels = var.labels)\r\n\r\n\r\n\r\n\r\nStata\r\n\r\nlabel variable hhind \"individual's ID\"\r\nlabel variable indid \"household's ID\" \r\nlabel variable year \"year\"\r\nlabel variable hhid \"individual's ID in the household\"\r\nlabel variable surname \"Surname\"\r\nlabel variable name \"Name\"\r\nlabel variable gender \"1 if male, 0 if female\"\r\nlabel variable wage \"wage\"\r\nlabel variable location \"household's location\"\r\nWe can see the result with:\r\n\r\n\r\ncontents(base_created2)\r\n\r\n\r\n\r\nData frame:base_created2    46 observations and 9 variables    Maximum # NAs:0\r\n\r\n                                   Labels     Class   Storage\r\nhhind                     individual's ID character character\r\nhhid                       household's ID   integer   integer\r\nindid    individual's ID in the household   integer   integer\r\nyear                                 year   integer   integer\r\nsurname                           surname character character\r\nname                                 name character character\r\ngender             1 if male, 0 if female   integer   integer\r\nwage                                 wage   integer   integer\r\nlocation             household's location character character\r\n\r\nNow that our dataframe is clean and detailed, we can compute some descriptive statistics. But before doing it, we might want to save it:\r\n\r\nwrite.xlsx(base_created2, file = here(\"Bases_Created/modified_base.xlsx\")\r\n\r\n\r\nStata\r\n\r\ncd ..\\Bases_Created  \r\nexport excel using \"modified_base.xls\", replace\r\nDescriptive Statistics\r\nFirst of all, if we want to check the number of people per location or gender and per year, we use the table function:\r\n\r\n\r\ntable(base_created2$gender, base_created2$year)\r\n\r\n\r\n   \r\n    2019 2020\r\n  0    9    9\r\n  1   14   14\r\n\r\ntable(base_created2$location, base_created2$year)\r\n\r\n\r\n         \r\n          2019 2020\r\n  England    6    6\r\n  France    12   12\r\n  Italy      1    1\r\n  Spain      4    4\r\n\r\n\r\nStata\r\n\r\ntab gender if year==2019  \r\ntab location if year==2019 \r\nTo have more detailed statistics, you can use many functions. Here, we use the function describe from the Hmisc package\r\n\r\n\r\ndescribe(base_created2)\r\n\r\n\r\nbase_created2 \r\n\r\n 9  Variables      46  Observations\r\n----------------------------------------------------------------------\r\nhhind : individual's ID \r\n       n  missing distinct \r\n      46        0       23 \r\n\r\nlowest : 11 12 13 14 21, highest: 71 72 81 82 83\r\n----------------------------------------------------------------------\r\nhhid : household's ID \r\n       n  missing distinct     Info     Mean      Gmd \r\n      46        0        8    0.975    4.217    2.783 \r\n\r\nlowest : 1 2 3 4 5, highest: 4 5 6 7 8\r\n                                                          \r\nValue          1     2     3     4     5     6     7     8\r\nFrequency      8     8     4     2    10     4     4     6\r\nProportion 0.174 0.174 0.087 0.043 0.217 0.087 0.087 0.130\r\n----------------------------------------------------------------------\r\nindid : individual's ID in the household \r\n       n  missing distinct     Info     Mean      Gmd \r\n      46        0        5    0.923    2.217    1.306 \r\n\r\nlowest : 1 2 3 4 5, highest: 1 2 3 4 5\r\n                                        \r\nValue          1     2     3     4     5\r\nFrequency     16    14     8     6     2\r\nProportion 0.348 0.304 0.174 0.130 0.043\r\n----------------------------------------------------------------------\r\nyear \r\n       n  missing distinct     Info     Mean      Gmd \r\n      46        0        2     0.75     2020   0.5111 \r\n                    \r\nValue      2019 2020\r\nFrequency    23   23\r\nProportion  0.5  0.5\r\n----------------------------------------------------------------------\r\nsurname \r\n       n  missing distinct \r\n      46        0       23 \r\n\r\nlowest : ANDERSON BROWN    DAVIS    DOE      JACKSON \r\nhighest: THOMAS   THOMPSON WHITE    WILLIAMS WILSON  \r\n----------------------------------------------------------------------\r\nname \r\n       n  missing distinct \r\n      46        0       23 \r\n\r\nlowest : Barbara Charles Daniel  David   Donald \r\nhighest: Richard Robert  Susan   Thomas  William\r\n----------------------------------------------------------------------\r\ngender : 1 if male, 0 if female \r\n       n  missing distinct     Info      Sum     Mean      Gmd \r\n      46        0        2    0.715       28   0.6087    0.487 \r\n\r\n----------------------------------------------------------------------\r\nwage \r\n       n  missing distinct     Info     Mean      Gmd      .05 \r\n      46        0       37    0.998     2059    477.4     1627 \r\n     .10      .25      .50      .75      .90      .95 \r\n    1692     1800     1901     2098     2373     3575 \r\n\r\nlowest : 1397 1600 1608 1683 1690, highest: 2384 3500 3600 3782 3784\r\n----------------------------------------------------------------------\r\nlocation : household's location \r\n       n  missing distinct \r\n      46        0        4 \r\n                                          \r\nValue      England  France   Italy   Spain\r\nFrequency       12      24       2       8\r\nProportion   0.261   0.522   0.043   0.174\r\n----------------------------------------------------------------------\r\n\r\n\r\nStata\r\n\r\nsum *, detail\r\nbut you can also try the function summary (automatically available in base R), stat.desc in pastecs, skim in skimr or even makeDataReport in dataMaid to have a complete PDF report summarizing your data. To summarize data under certain conditions (e.g. to have the average wage for each location), you can use dplyr:\r\n\r\n\r\n# you can change the argument in group_by() by gender for example\r\nbase_created2 %>%\r\n  group_by(location) %>%\r\n  summarize_at(.vars = \"wage\", .funs = \"mean\")\r\n\r\n\r\n# A tibble: 4 x 2\r\n  location    wage\r\n  <labelled> <dbl>\r\n1 England    2452.\r\n2 France     1935.\r\n3 Italy      1801 \r\n4 Spain      1905.\r\n\r\n\r\nStata\r\n\r\ntabstat wage if year==2019, stats(N mean sd min max p25 p50 p75) by(location)\r\ntabstat wage if year==2020, stats(N mean sd min max p25 p50 p75) by(location)\r\nPlots\r\nFinally, we want to plot some data to include in our report or article (or anything else). ggplot2 is THE reference to make plots with R. The ggplot function does not create a graph but tells what is the data you are going to use and the aesthetics (aes). Here, we want to display the wages in a histogram and to distinguish them per year. Therefore, we want to fill the bars according to the year. To precise the type of graph we want, we add + geom_histogram() after ggplot. You may change the number of bins to have a more precise histogram.\r\n\r\n\r\nlibrary(ggplot2)\r\nhist1 <- ggplot(data = base_created2, \r\n                mapping = aes(wage, fill = factor(year))) +\r\n  geom_histogram(bins = 10)\r\nhist1\r\n\r\n\r\n\r\n\r\n\r\nStata\r\n\r\nhistogram wage if year==2019, saving(Hist1, replace) bin(10) freq title(\"Year 2019\") ytitle(\"Frequency\") \r\nhistogram wage if year==2020, saving(Hist2, replace) bin(10) freq title(\"Year 2020\") ytitle(\"Frequency\")\r\nIf you prefer one histogram per year, you can use the facet_wrap() argument, as below.\r\n\r\n\r\nhist2 <- ggplot(data = base_created2, \r\n                mapping = aes(wage, fill = factor(year))) +\r\n  geom_histogram(bins = 10) +\r\n  facet_wrap(vars(year))\r\nhist2\r\n\r\n\r\n\r\n\r\n\r\nStata\r\n\r\ngraph combine Hist1.gph Hist2.gph, col(2) xsize(10) ysize(5) iscale(1.5) title(\"{bf:Wage distribution per year}\")\r\nFinally, you may want to export these graphs. To do so, we use ggsave (you can replace .pdf by .eps or .png if you want):\r\n\r\n\r\nggsave(here(\"Figures/plot1.pdf\"), plot = hist1)\r\n\r\n\r\n\r\n\r\nStata\r\n\r\ngraph export Histogram1.pdf,  replace\r\nThat’s it! In this first post, you have seen how to import, clean and tidy datasets, and how to make some descriptive statistics and some plots. I hope this was helpful to you!\r\n\r\nSession Info\r\nThis is my session info, so that you can see the versions of packages used. This is useful if the results in my post are no longer reproducible because packages changed. The packages with a star (*) are those explicitely called in the script.\r\n\r\n─ Session info ─────────────────────────────────────────────────────\r\n setting  value                       \r\n version  R version 4.0.4 (2021-02-15)\r\n os       Ubuntu 18.04.5 LTS          \r\n system   x86_64, linux-gnu           \r\n ui       X11                         \r\n language en                          \r\n collate  fr_FR.UTF-8                 \r\n ctype    fr_FR.UTF-8                 \r\n tz       Europe/Paris                \r\n date     2021-03-16                  \r\n\r\n─ Packages ─────────────────────────────────────────────────────────\r\n package      * version date       lib\r\n assertthat     0.2.1   2019-03-21 [1]\r\n backports      1.2.1   2020-12-09 [1]\r\n base64enc      0.1-3   2015-07-28 [1]\r\n bslib          0.2.4   2021-01-25 [1]\r\n cellranger     1.1.0   2016-07-27 [1]\r\n checkmate      2.0.0   2020-02-06 [1]\r\n cli            2.3.1   2021-02-23 [1]\r\n cluster        2.1.1   2021-02-14 [4]\r\n colorspace     2.0-0   2020-11-11 [1]\r\n crayon         1.4.1   2021-02-08 [1]\r\n data.table     1.13.6  2020-12-30 [1]\r\n DBI            1.1.1   2021-01-15 [1]\r\n debugme        1.1.0   2017-10-22 [1]\r\n digest         0.6.27  2020-10-24 [1]\r\n distill        1.2.2   2021-03-04 [1]\r\n downlit        0.2.1   2020-11-04 [1]\r\n dplyr        * 1.0.5   2021-03-05 [1]\r\n ellipsis       0.3.1   2020-05-15 [1]\r\n evaluate       0.14    2019-05-28 [1]\r\n fansi          0.4.2   2021-01-15 [1]\r\n farver         2.0.3   2020-01-16 [1]\r\n foreign        0.8-81  2020-12-22 [4]\r\n Formula      * 1.2-4   2020-10-16 [1]\r\n generics       0.1.0   2020-10-31 [1]\r\n ggplot2      * 3.3.3   2020-12-30 [1]\r\n glue           1.4.2   2020-08-27 [1]\r\n gridExtra      2.3     2017-09-09 [1]\r\n gtable         0.3.0   2019-03-25 [1]\r\n here         * 1.0.1   2020-12-13 [1]\r\n highr          0.8     2019-03-20 [1]\r\n Hmisc        * 4.4-2   2020-11-29 [1]\r\n htmlTable      2.1.0   2020-09-16 [1]\r\n htmltools      0.5.1.1 2021-01-22 [1]\r\n htmlwidgets    1.5.3   2020-12-10 [1]\r\n jpeg           0.1-8.1 2019-10-24 [1]\r\n jquerylib      0.1.3   2020-12-17 [1]\r\n jsonlite       1.7.2   2020-12-09 [1]\r\n knitr          1.31    2021-01-27 [1]\r\n labeling       0.4.2   2020-10-20 [1]\r\n lattice      * 0.20-41 2020-04-02 [4]\r\n latticeExtra   0.6-29  2019-12-19 [1]\r\n lifecycle      1.0.0   2021-02-15 [1]\r\n magrittr       2.0.1   2020-11-17 [1]\r\n Matrix         1.3-2   2021-01-06 [4]\r\n munsell        0.5.0   2018-06-12 [1]\r\n nnet           7.3-15  2021-01-24 [4]\r\n pillar         1.5.1   2021-03-05 [1]\r\n pkgconfig      2.0.3   2019-09-22 [1]\r\n png            0.1-7   2013-12-03 [1]\r\n purrr          0.3.4   2020-04-17 [1]\r\n R6             2.5.0   2020-10-28 [1]\r\n RColorBrewer   1.1-2   2014-12-07 [1]\r\n Rcpp           1.0.6   2021-01-15 [1]\r\n readxl       * 1.3.1   2019-03-13 [1]\r\n rlang          0.4.10  2020-12-30 [1]\r\n rmarkdown      2.6.6   2021-02-08 [1]\r\n rpart          4.1-15  2019-04-12 [4]\r\n rprojroot      2.0.2   2020-11-15 [1]\r\n rstudioapi     0.13    2020-11-12 [1]\r\n sass           0.3.1   2021-01-24 [1]\r\n scales         1.1.1   2020-05-11 [1]\r\n sessioninfo    1.1.1   2018-11-05 [1]\r\n stringi        1.5.3   2020-09-09 [1]\r\n stringr        1.4.0   2019-02-10 [1]\r\n survival     * 3.2-7   2020-09-28 [4]\r\n tibble         3.1.0   2021-02-25 [1]\r\n tidyr        * 1.1.2   2020-08-27 [1]\r\n tidyselect     1.1.0   2020-05-11 [1]\r\n utf8           1.2.1   2021-03-12 [1]\r\n vctrs          0.3.6   2020-12-17 [1]\r\n withr          2.4.1   2021-01-26 [1]\r\n xfun           0.20    2021-01-06 [1]\r\n yaml           2.2.1   2020-02-01 [1]\r\n source                            \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.4)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.2)                    \r\n CRAN (R 4.0.3)                    \r\n local                             \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.4)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.2)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.4)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.4)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.3)                    \r\n Github (rstudio/rmarkdown@de0e2ec)\r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.4)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.2)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.2)                    \r\n CRAN (R 4.0.4)                    \r\n CRAN (R 4.0.2)                    \r\n CRAN (R 4.0.0)                    \r\n CRAN (R 4.0.4)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.3)                    \r\n CRAN (R 4.0.0)                    \r\n\r\n[1] /home/etienne/R/x86_64-pc-linux-gnu-library/4.0\r\n[2] /usr/local/lib/R/site-library\r\n[3] /usr/lib/R/site-library\r\n[4] /usr/lib/R/library\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-01-22-first-contact/distill-preview.png",
    "last_modified": "2022-05-03T17:54:55+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2019-12-01-why-moving/",
    "title": "Why you should move from Stata to R",
    "description": "Some reasons that explain why I prefer R to Stata.",
    "author": [
      {
        "name": "Etienne Bacher",
        "url": {}
      }
    ],
    "date": "2019-12-01",
    "categories": [],
    "contents": "\r\nBefore going into the details about studying economics with R, it makes sense to explain why you should use R compared to Stata. Before I start, please note that I have been using Stata occasionally for about a year whereas I spend much more time on R so I may forget some features that Stata has and that I am not aware of. However, I believe that what I have made with Stata corresponds to most Master students’ experiences, e.g. data cleaning and treatment, data analysis, econometrics, etc.\r\nNow we can begin.\r\nReason 1: R is free\r\nThat may seem a false argument for some people, especially because in many universities, students have freely access to Stata. However, in my experience, I know that we frequently want to work home or in group on some projects and therefore we need Stata on our personal laptop. Therefore, some cracked versions circulate between students and it is well-known that when downloading illegally softwares (and movies, TV shows, etc.), there’s always a risk of being infected by a virus. I don’t know if this happens often or not, maybe you will never suffer from it, but it would be just stupid to have to suffer from a hacking just because the statistical software was not free. That’s the big advantage of R: it is completely free. Whatever your operating system, you can download base R and every package you want and it won’t cost any money.\r\nReason 2: R is open-source\r\nI have already heard one of my professor complaining about the fact that Stata is a “black box” (not like those in planes but more like an opaque system). On the contrary, R is open-source (meaning that anyone can see the code, contribute to it and distribute it) and the code behind the functions you use is easily visible with just one click. That accessiblity entails the next argument, which is the diversity of packages.\r\nReason 3: the diversity of packages\r\nThere is A LOT of packages on R (more than 10,000 on CRAN as shown here, and it was in 2017!). Additionally to the packages on CRAN (the Comprehensive R Archive Network, where the stable versions of the packages are), some packages are hosted only on Github and others are made by users or companies only for private purposes and will not be released on open-source. The packages are the strength of R. Base-R (i.e. the basic version of R, without any packages manually installed) is a great start to learn how to code and to manipulate data, and in fact you can stay with base-R only if you limit your study to some basic data analysis. However, base-R may also be hard to learn and not very esthetic. Moreover, some packages allow to extend R functionalities beyond base-R.\r\nThis is a list (far from being exhaustive) of some of the most important packages for students in economics:\r\ntidyverse: this is a portmanteau word of tidy and universe. It regroups more than 20 packages for data import (readr), data treatment (dplyr, tidyr…), graphics (ggplot2), etc.\r\nrmarkdown: as a student (in economics but in other domains too), you will have to write a Master’s thesis and before that, you will certainly have to do some group projects, sort of small reports. When the data analysis will be done, you will have to write your report and to incorporate the results in the document. That can lead to some mistakes/typos that can lead to big errors, like changes in p-values between the results obtained in the statistical software and the word processing program (whether it is LaTeX or Microsoft Word). To guarantee that you won’t make this sort of mistakes, the most effective way is to write directly in R and to incorporate your code directly in the text. Therefore, in one document, you will have the text of your report and the code needed for the data analysis, all of that ready to be converted in PDF, HTML or Word.\r\nshiny: while rmarkdown promotes reproducibility by keeping all in a unique document, shiny goes a step further. Once you have made some data analysis, you can put it in a Shiny application (“app”) that will create a web page in which people interested in your work will be able to reproduce your results but also to check wether they are robust. Indeed, Shiny makes results reactive, meaning that you can change the sample size or the years or anything else you want and the results will automatically adapt. That is very useful in econometrics, where robustness is very important and always checked. There exist thousands and thousands of R packages which cover a large spectrum of the problems and questions you might have, and that is definitely a strength of R.\r\nReason 4: the community\r\nIt is certain that will have some problems with your code, everybody has. The documentation is very complete and allows to solve most of them, but sometimes you may need to seek for help online. It is quite probable that the question you ask yourself has already been asked by somebody else before you and if it has, you will find the answer on StackOverflow or on the RStudio Community.\r\nReason 5: RStudio is just a pleasure to use\r\nRStudio is the most used IDE for R (Integrated Desktop Environment, not the language but a software that permits to use more easily the language). It has tons of shortcuts and is very customizable. It is a real pleasure to use, and it can be linked to other great services like GitHub (maybe you don’t know what it is so in a few words, it is a service that permits version control i.e. keep a trace of every change in a project, whether it is a report, a package or a web application).\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-03T17:54:55+02:00",
    "input_file": {}
  }
]
